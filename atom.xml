<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>风向标 | 分享与创造</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://sbcoder.cn/"/>
  <updated>2019-04-12T10:56:18.000Z</updated>
  <id>https://sbcoder.cn/</id>
  
  <author>
    <name>ai0by</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Discuz会员数据与Wordpress互通</title>
    <link href="https://sbcoder.cn/2019/04/11/Discuz-Userinfo-To-Wordpress.html"/>
    <id>https://sbcoder.cn/2019/04/11/Discuz-Userinfo-To-Wordpress.html</id>
    <published>2019-04-11T12:06:06.000Z</published>
    <updated>2019-04-12T10:56:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="情景"><a href="#情景" class="headerlink" title="情景"></a>情景</h2><p>这个情景可能遇到的也不在少数，不想舍弃用户数据，还想让用户无需注册在新站保留账号。<br>实际当我们在迁移的时候，稍微了解数据库的同学应该明白想要迁移用户数据只需要迁移用户数据表即可。<br>实际上我也是这么做的，但是中途遇到了几个小问题，这里我总结一下！</p><h2 id="Discuz用户密码加密算法"><a href="#Discuz用户密码加密算法" class="headerlink" title="Discuz用户密码加密算法"></a>Discuz用户密码加密算法</h2><p>Discuz的用户信息都存放在 ‘<font color="#FF0000">pre_common_member</font>‘  表里，包含了我们需要转移的 邮箱,用户名,密码,积分,ip 等各类信息<br>那么很简单了，便利这个表再插入到Wordpress表内即可<br>但在导入表之前需要先测试一下用户数据是否匹配以示严谨~<br>当我测试密码匹配的时候发现，这里的密码似乎并不匹配，首先我想到的就是应该是加盐了，但是纵观整个 ‘<font color="#FF0000">pre_common_member</font>‘ 表，似乎并没有该有的字段<br>网上找了一圈发现Discuz的用户真实密码是存在 ‘<font color="#FF0000">pre_ucenter_members</font>‘ 表内的，’<font color="#FF0000">pre_common_member</font>‘ 表内的密码我现在还不知道有什么用处，但至少跟我们需要迁移的数据没什么关联。<br>从  ‘<font color="#FF0000">pre_ucenter_members</font>‘ 表中找到了我们需要的 ‘<font color="#FF0000">salt</font>‘ 字段，经过测试得出Discuz的密码加密算法为<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">md5(md5(<span class="string">'password'</span>).<span class="string">'salt'</span>);</span><br></pre></td></tr></table></figure></p><p>tips: Discuz里面的salt是一个6位的 数字+字母 随机数</p><h2 id="Wordpress用户密码加密算法"><a href="#Wordpress用户密码加密算法" class="headerlink" title="Wordpress用户密码加密算法"></a>Wordpress用户密码加密算法</h2><p>搞定了DZ的加密算法后，那么如何将DZ的用户信息插入到WP里面就很重要了，打开 Wordpress 的数据库找到 ‘<font color="#FF0000">wp-user</font>‘表，找到 ‘<font color="#FF0000">user_pass</font>‘ 字段，发现里面加密的内容似乎无迹可寻。<br>实际上，Wordpress的加密是使用了 phpass 类来加密的，由 phpass 加密的密码具有不可逆性，所以想要破解是不可能了，这里简单说一下 phpass 的加密算法<br>目前我们的PHP版本应该都在5以上，所以前缀是一样的 $P$B 大致写出来如下：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$count = rand(<span class="number">1</span>,<span class="number">8</span>);</span><br><span class="line">$hash = md5($salt . $password, <span class="keyword">TRUE</span>);</span><br><span class="line"><span class="keyword">while</span>($count--)&#123;</span><br><span class="line">$hash = md5($hash . $password, <span class="keyword">TRUE</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>看起来是不是很强，我们无法破解这样的密码，实际使用中，我们也可以使用 phpass 来做密码加密，让我们的数据库更加的安全~<br>然而我们数据迁移时其实完全可以避免这种问题，Wordpress是保留了md5加密的形式的，如果 ‘<font color="#FF0000">user_pass</font>‘ 字段里面存储的是md5加密的32值，wordpress也可以登录成功，并且再登陆后会将 ‘<font color="#FF0000">user_pass</font>‘ 字段修改为 phpass 加密的格式，是不是很人性化呢。<br>综上所述，我们无需解出来wordpress的加密算法，我们也解不出来~</p><h2 id="开始迁移用户数据"><a href="#开始迁移用户数据" class="headerlink" title="开始迁移用户数据"></a>开始迁移用户数据</h2><p>未完待续…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;情景&quot;&gt;&lt;a href=&quot;#情景&quot; class=&quot;headerlink&quot; title=&quot;情景&quot;&gt;&lt;/a&gt;情景&lt;/h2&gt;&lt;p&gt;这个情景可能遇到的也不在少数，不想舍弃用户数据，还想让用户无需注册在新站保留账号。&lt;br&gt;实际当我们在迁移的时候，稍微了解数据库的同学应该明
      
    
    </summary>
    
      <category term="PHP" scheme="https://sbcoder.cn/categories/PHP/"/>
    
    
      <category term="Discuz" scheme="https://sbcoder.cn/tags/Discuz/"/>
    
      <category term="Wordpress" scheme="https://sbcoder.cn/tags/Wordpress/"/>
    
  </entry>
  
  <entry>
    <title>192TT(192tb)套图吧整站爬虫</title>
    <link href="https://sbcoder.cn/2019/03/28/192tt_Spider.html"/>
    <id>https://sbcoder.cn/2019/03/28/192tt_Spider.html</id>
    <published>2019-03-28T08:15:16.000Z</published>
    <updated>2019-03-29T06:33:44.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="观察目录结构"><a href="#观察目录结构" class="headerlink" title="观察目录结构"></a>观察目录结构</h2><p>目标网站：192tb.com<br>网站结构复杂，但也不是太复杂，总体来说都写在导航栏上面了，本以为是new分类下就是所有的文章了，后来发现不是，需要遍历整个导航分类，由于每个分类都有很庞大的资源，因此我决定写成配置文件的形式，建立config.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">mt  = <span class="string">'https://www.192tb.com/listinfo-1-1.html'</span>                  <span class="comment"># 美图</span></span><br><span class="line">mt1 = <span class="string">'https://www.192tb.com/meitu/xingganmeinv/'</span>  <span class="comment"># 性感美女</span></span><br><span class="line">mt2 = <span class="string">'https://www.192tb.com/meitu/siwameitui/'</span>    <span class="comment"># 丝袜美腿</span></span><br><span class="line">mt3 = <span class="string">'https://www.192tb.com/meitu/weimeixiezhen/'</span> <span class="comment"># 唯美写真</span></span><br><span class="line">mt4 = <span class="string">'https://www.192tb.com/meitu/wangluomeinv/'</span>  <span class="comment"># 网络美女</span></span><br><span class="line">mt5 = <span class="string">'https://www.192tb.com/meitu/gaoqingmeinv/'</span>  <span class="comment"># 高清美女</span></span><br><span class="line">mt6 = <span class="string">'https://www.192tb.com/meitu/motemeinv/'</span>     <span class="comment"># 模特美女</span></span><br><span class="line">mt7 = <span class="string">'https://www.192tb.com/meitu/tiyumeinv/'</span>     <span class="comment"># 体育美女</span></span><br><span class="line">mt8 = <span class="string">'https://www.192tb.com/meitu/dongmanmeinv/'</span>  <span class="comment"># 动漫美女</span></span><br><span class="line">mt9 = <span class="string">'https://www.192tb.com/new/ugirlapp/'</span>        <span class="comment"># 爱尤物APP/尤果网</span></span><br><span class="line"></span><br><span class="line">gc  = <span class="string">'https://www.192tb.com/gc/'</span>                   <span class="comment"># 国产</span></span><br><span class="line">gc1 = <span class="string">'https://www.192tb.com/gc/bl/'</span> <span class="comment"># beautyleg1</span></span><br></pre></td></tr></table></figure><p>顶级分类和二级分类不便多说，这里只是测试并没有收录所有的分类，有兴趣可以自己添加</p><p>进入分类页后既是套图封面，从这里可以爬取套图的链接，分类页的底部也是有下一页的选项，可以根据下一页来获取下一个分类页的链接，以此递归，并获取链接</p><p>获取到套图链接后发现每个单页面都是需要点击下一张图片来做的，单页面中的图片，使用BeautifulSoup即可轻松获取，由于不知道一套图里面有多少张，我这边使用递归的方式，走到最后一张，即退出递归。</p><h2 id="核心代码"><a href="#核心代码" class="headerlink" title="核心代码"></a>核心代码</h2><h3 id="获取单个套图并下载"><a href="#获取单个套图并下载" class="headerlink" title="获取单个套图并下载"></a>获取单个套图并下载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSingleData</span><span class="params">(url,singleTitle,i = <span class="number">1</span>)</span>:</span></span><br><span class="line">    response = requests.get(url)</span><br><span class="line">    soup = BeautifulSoup(response.text,<span class="string">"html.parser"</span>)</span><br><span class="line">    imgUrl = soup.find(id = <span class="string">'p'</span>).find(<span class="string">'center'</span>).find(<span class="string">'img'</span>).get(<span class="string">'lazysrc'</span>)</span><br><span class="line">    <span class="keyword">print</span> imgUrl</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        j = i + <span class="number">1</span></span><br><span class="line">        result = <span class="string">'_%s.html'</span>%i <span class="keyword">in</span> url</span><br><span class="line">        <span class="keyword">if</span> result:</span><br><span class="line">            nextImg = response.url.replace(<span class="string">'_%s.html'</span>%i, <span class="string">'_%s.html'</span>%j)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nextImg = response.url.replace(<span class="string">'.html'</span>, <span class="string">'_%s.html'</span>%j)</span><br><span class="line">        <span class="comment"># print nextImg</span></span><br><span class="line">        downImg(imgUrl,singleTitle,i)</span><br><span class="line">        getSingleData(nextImg,j)</span><br><span class="line">    <span class="keyword">except</span> Exception,e:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="获取下一页页面信息"><a href="#获取下一页页面信息" class="headerlink" title="获取下一页页面信息"></a>获取下一页页面信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPage</span><span class="params">(url,new = <span class="number">1</span>,i = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'开始采集第%s页'</span>%i</span><br><span class="line">    <span class="keyword">print</span> url</span><br><span class="line">    response = requests.get(url)</span><br><span class="line">    soup = BeautifulSoup(response.text,<span class="string">"html.parser"</span>)</span><br><span class="line">    <span class="keyword">for</span> dataUrl <span class="keyword">in</span> soup.find(<span class="string">'div'</span>,&#123;<span class="string">'class'</span>:<span class="string">'piclist'</span>&#125;).find(<span class="string">'ul'</span>).find_all(<span class="string">'li'</span>):</span><br><span class="line">        singleDataUrl = <span class="string">'https://www.192tb.com/'</span>+dataUrl.find(<span class="string">'a'</span>).get(<span class="string">'href'</span>)</span><br><span class="line">        <span class="keyword">print</span> singleDataUrl</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            singleTitle = dataUrl.find(<span class="string">'a'</span>).find(<span class="string">'img'</span>).get(<span class="string">'alt'</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception,e:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">print</span> singleTitle</span><br><span class="line">        getSingleData(singleDataUrl,singleTitle)</span><br><span class="line">    result = <span class="string">'_%s.html'</span> % i <span class="keyword">in</span> url</span><br><span class="line">    j = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> new != <span class="number">1</span>:</span><br><span class="line">        nextPageUrl = url.replace(<span class="string">'listinfo-1-%s.html'</span> % i, <span class="string">'listinfo-1-%s.html'</span> % j)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> result:</span><br><span class="line">            nextPageUrl = url.replace(<span class="string">'index_%s.html'</span> % i, <span class="string">'index_%s.html'</span> % j)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nextPageUrl = url.replace(url, url+<span class="string">'/index_%s.html'</span> % j)</span><br><span class="line">    getPage(nextPageUrl,new,j)</span><br></pre></td></tr></table></figure><h2 id="演示及下载"><a href="#演示及下载" class="headerlink" title="演示及下载"></a>演示及下载</h2><p>下载地址：<a href="https://github.com/ai0by/ai0by-spider/tree/master/192tt" target="_blank" rel="noopener">Github</a></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://fulicos.sbcoder.cn/2019/03/29/5c9dba3205190.png" alt="192tt演示图" title>                </div>                <div class="image-caption">192tt演示图</div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;观察目录结构&quot;&gt;&lt;a href=&quot;#观察目录结构&quot; class=&quot;headerlink&quot; title=&quot;观察目录结构&quot;&gt;&lt;/a&gt;观察目录结构&lt;/h2&gt;&lt;p&gt;目标网站：192tb.com&lt;br&gt;网站结构复杂，但也不是太复杂，总体来说都写在导航栏上面了，本以为是new
      
    
    </summary>
    
      <category term="Python" scheme="https://sbcoder.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="https://sbcoder.cn/tags/Python/"/>
    
      <category term="爬虫" scheme="https://sbcoder.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="福利" scheme="https://sbcoder.cn/tags/%E7%A6%8F%E5%88%A9/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式应用，常用取值表（记录）</title>
    <link href="https://sbcoder.cn/2019/03/26/Regex_match_note.html"/>
    <id>https://sbcoder.cn/2019/03/26/Regex_match_note.html</id>
    <published>2019-03-26T08:11:06.000Z</published>
    <updated>2019-03-27T03:50:12.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正则表达式-查询表"><a href="#正则表达式-查询表" class="headerlink" title="正则表达式 查询表"></a>正则表达式 查询表</h2><table><thead><tr><th>字符</th><th>描述</th><th>场景  </th></tr></thead><tbody><tr><td> \</td><td>转义</td><td>转义场景 \</td></tr><tr><td>^</td><td>匹配输入字符串的开始位置。如果设置了 RegExp 对象的 Multiline 属性，^ 也匹配 ‘\n’ 或 ‘\r’ 之后的位置。</td><td>取a开头的字符串 ^a.*</td></tr><tr><td>$</td><td>匹配输入字符串的结束位置。如果设置了RegExp 对象的 Multiline 属性，$ 也匹配 ‘\n’ 或 ‘\r’ 之前的位置。</td><td>取a开头b结尾 ^a.*b$</td></tr><tr><td>*</td><td>匹配前面的子表达式零次或多次。</td><td>zo<em> 能匹配 “z” 以及 “zoo”。</em> 等价于{0,}</td></tr><tr><td>+</td><td>匹配前面的子表达式一次或多次。</td><td>‘zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,}</td></tr><tr><td>?</td><td>匹配前面的子表达式零次或一次.</td><td>“do(es)?” 可以匹配 “do” 或 “does” 中的”do” 。? 等价于 {0,1}。</td></tr><tr><td>{n}</td><td>n 是一个非负整数。匹配确定的 n 次。</td><td>‘o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。</td></tr><tr><td>{n,}</td><td>n 是一个非负整数。至少匹配n 次。</td><td>‘o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。’o{1,}’ 等价于 ‘o+’。’o{0,}’ 则等价于 ‘o*’</td></tr><tr><td>{n,m}</td><td>m 和 n 均为非负整数，其中n &lt;= m。最少匹配 n 次且最多匹配 m 次。</td><td>“o{1,3}” 将匹配 “fooooood” 中的前三个 o。’o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。</td></tr><tr><td>?</td><td>当 该字符紧跟在任何一个其他限制符 (*, +, ?, {n}, {n,}, {n,m}) 后面时，匹配模式是非贪婪的。</td><td>非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。对于字符串 “oooo”，’o+?’ 将匹配单个 “o”，而 ‘o+’ 将匹配所有 ‘o’。</td></tr><tr><td>.</td><td>匹配除 “\n” 之外的任何单个字符。</td><td>要匹配包括 ‘\n’ 在内的任何字符，请使用象 ‘[.\n]’ 的模式。</td></tr><tr><td>x&#124;y</td><td>匹配 x 或 y。</td><td>‘z&#124;food’ 能匹配 “z” 或 “food”。’(z&#124;f)ood’ 则匹配 “zood” 或 “food”。</td></tr><tr><td>[xyz]</td><td>字符集合。匹配所包含的任意一个字符。</td><td>‘[abc]’可以匹配 “plain” 中的 ‘a’。</td></tr><tr><td>[^xyz]</td><td>取反，匹配未包含的任意字符。</td><td>‘?[^abc]’ 可以匹配 “plain” 中的’p’。</td></tr><tr><td>[a-z]</td><td>字符范围。匹配指定范围内的任意字符。</td><td>‘[a-z]’ 可以匹配 ‘a’ 到 ‘z’ 范围内的任意小写字母字符。</td></tr><tr><td>\b</td><td>匹配一个单词边界，也就是指单词和空格间的位置。</td><td>‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。</td></tr><tr><td>\B</td><td>匹配非单词边界</td><td>‘er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。</td></tr><tr><td>\cx</td><td>匹配由 x 指明的控制字符。</td><td>\cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。</td></tr><tr><td>\d</td><td>匹配一个数字字符</td><td>等价于 [0-9]。</td></tr><tr><td>\D</td><td>匹配一个非数字字符。</td><td>等价于 [^0-9]。</td></tr><tr><td>\f</td><td>匹配一个换页符。</td><td>等价于 \x0c 和 \cL。</td></tr><tr><td>\n</td><td>匹配一个换行符。</td><td>等价于 \x0a 和 \cJ。</td></tr><tr><td>\r</td><td>匹配一个回车符。</td><td>等价于 \x0d 和 \cM。</td></tr><tr><td>\s</td><td>匹配任何空白字符，包括空格、制表符、换页符等等。</td><td>等价于 [ \f\n\r\t\v]。</td></tr><tr><td>\S</td><td>匹配任何非空白字符。</td><td>等价于 [^ \f\n\r\t\v]。</td></tr><tr><td>\t</td><td>匹配一个制表符。</td><td>等价于 \x09 和 \cI。</td></tr><tr><td>\v</td><td>匹配一个垂直制表符。</td><td>等价于 \x0b 和 \cK。</td></tr><tr><td>\w</td><td>匹配包括下划线的任何单词字符。</td><td>等价于’[A-Za-z0-9_]’。</td></tr><tr><td>\W</td><td>匹配任何非单词字符。</td><td>等价于 ‘[^A-Za-z0-9_]’。</td></tr><tr><td>\xn</td><td>匹配十六进制数</td><td>‘\x41’ 匹配 “A”。’\x041’ 则等价于 ‘\x04’ &amp; “1”。正则表达式中可以使用 ASCII 编码。.</td></tr><tr><td>\num</td><td>匹配 一个正整数。对所获取的匹配的引用。</td><td>‘(.)\1’ 匹配两个连续的相同字符。</td></tr><tr><td>\n</td><td>标识一个八进制转义值或一个向后引用。</td><td>如果 \n 之前至少 n 个获取的子表达式，则 n 为向后引用。否则，如果 n 为八进制数字 (0-7)，则 n 为一个八进制转义值。</td></tr><tr><td>\nm</td><td>标 识一个八进制转义值或一个向后引用。</td><td>如果 \nm 之前至少有 nm 个获得子表达式，则 nm 为向后引用。如果 \nm 之前至少有 n 个获取，则 n 为一个后跟文字 m 的向后引用。如果前面的条件都不满足，若 n 和 m 均为八进制数字 (0-7)，则 \nm 将匹配八进制转义值 nm。</td></tr><tr><td>\nml</td><td>匹配八进制数</td><td>如果 n 为八进制数字 (0-3)，且 m 和 l 均为八进制数字 (0-7)，则匹配八进制转义值 nml。</td></tr><tr><td>\un</td><td>匹配 n，其中 n 是一个用四个十六进制数字表示的 Unicode 字符。</td><td>\u00A9 匹配版权符号 。</td></tr></tbody></table><h2 id="常用案例演示"><a href="#常用案例演示" class="headerlink" title="常用案例演示"></a>常用案例演示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">str1 = <span class="string">'ai0by123'</span></span><br></pre></td></tr></table></figure><p>提取a开头的字符串<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regexStr = <span class="string">"^a.*"</span></span><br></pre></td></tr></table></figure></p><p>提取a开头b结尾字符串<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regexStr = <span class="string">"^a.*3$"</span></span><br></pre></td></tr></table></figure></p><p>提取最右边符合条件的值,贪婪<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regexStr = <span class="string">".*(a.*b).*"</span>             <span class="comment"># 贪婪，取a到b之间，右边开始取，取最右边符合条件的</span></span><br></pre></td></tr></table></figure></p><p>提取最左边符合条件的值，非贪婪<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regexStr = <span class="string">".*?(a.*?b).*"</span>           <span class="comment"># 非贪婪，取a到b之间的值含a和b，从左往右只取一次</span></span><br></pre></td></tr></table></figure></p><p>提取符合集合内的值，或运算<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regexStr = <span class="string">"((ai00000by|ai0by)123)"</span> <span class="comment"># 或运算，符合其中一种即可</span></span><br></pre></td></tr></table></figure></p><p>提取出生日期<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">str1 = <span class="string">'XXX 出生于2008年12月6日'</span></span><br><span class="line">str1 = <span class="string">'XXX 出生于2008/12/6'</span></span><br><span class="line">str1 = <span class="string">'XXX 出生于2008-12-6'</span></span><br><span class="line">str1 = <span class="string">'XXX 出生于2008-12-06'</span></span><br><span class="line">str1 = <span class="string">'XXX 出生于2008-12'</span></span><br><span class="line">regexStr = <span class="string">".*出生于(\d&#123;4&#125;[年/-]\d&#123;1,2&#125;([月/-]\d&#123;1,2&#125;|[月/-]$|$))"</span></span><br></pre></td></tr></table></figure></p><p>提取图片url,其他网站同理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">str1 = <span class="string">'地址：https://www.ttbcdn.com/d/file/p/2018-02-17/g4edlvxmmyi9627.jpg'</span></span><br><span class="line"><span class="comment"># 取整串地址</span></span><br><span class="line">regexStr = <span class="string">".*https.*jpg$"</span></span><br><span class="line"><span class="comment"># 取XXX.jpg png gif 等</span></span><br><span class="line">regexStr = <span class="string">".*/(.*.(jpg|gif|png))$"</span></span><br><span class="line"><span class="comment"># 取2018-02-17/g4edlvxmmyi9627.jpg png gif等</span></span><br><span class="line">regexStr = <span class="string">".*/(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;/(.*.(jpg|gif|png)))$"</span></span><br></pre></td></tr></table></figure></p><h2 id="收尾提取字符串"><a href="#收尾提取字符串" class="headerlink" title="收尾提取字符串"></a>收尾提取字符串</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">reMatch = re.match(regexStr,str1)</span><br><span class="line"><span class="keyword">if</span> reMatch:</span><br><span class="line"><span class="keyword">print</span> (reMatch.group(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">'No'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;正则表达式-查询表&quot;&gt;&lt;a href=&quot;#正则表达式-查询表&quot; class=&quot;headerlink&quot; title=&quot;正则表达式 查询表&quot;&gt;&lt;/a&gt;正则表达式 查询表&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;字符&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;

      
    
    </summary>
    
      <category term="note" scheme="https://sbcoder.cn/categories/note/"/>
    
    
      <category term="正则表达式" scheme="https://sbcoder.cn/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
      <category term="笔记" scheme="https://sbcoder.cn/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>博客迁移说明</title>
    <link href="https://sbcoder.cn/2019/03/25/Hello_world.html"/>
    <id>https://sbcoder.cn/2019/03/25/Hello_world.html</id>
    <published>2019-03-25T05:12:58.000Z</published>
    <updated>2019-04-12T14:57:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于迁移"><a href="#关于迁移" class="headerlink" title="关于迁移"></a>关于迁移</h2><p>博客总是在起起伏伏，关了又开，开了又关中反复，这一次，我将风向标博客放在了Github Page上。<br>程序采用了当下比较流行的静态博客程序 Hexo ，Hexo其实是一个非常好的程序，但由于我经常换电脑，以前用hexo搭建的博客数据丢失了很多次，后经过更换为Wordpress，Typecho之类的开源博客程序后，我又回到了 Hexo 的怀抱，可能是真的懒得折腾了，上了年纪？<br>这次我将源代码都备份好了，应该会长期更新，有什么好的东西我应该会分享出来，主打原创~<br>可能之前认识我的人也很少，但我这个域名还是很好记的，sb coder 也是一种自嘲吧，有想跟我交流技术或者有外包工作介绍给我的，我的微信与域名同号~<br>多的不说了，我将尽我所能，一周至少写一篇文章，可能有时候晚上回家写一点，一天写一点，一周下来也能写不少，希望各位监督~</p><h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><p>我是谁，职业是PHP，爱好Python，云服务器爱好者<br>支付接口对接，可以定制各类免签约支付接口，微信支付宝，有想法的朋友可以联系我 Telegram : ai0by</p><h2 id="承接业务"><a href="#承接业务" class="headerlink" title="承接业务"></a>承接业务</h2><p>支付相关业务，爬虫(几乎不要钱，练手)，PHP程序开发<br>服务器环境配置，PHP程序修改，PHP BUG排查</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;关于迁移&quot;&gt;&lt;a href=&quot;#关于迁移&quot; class=&quot;headerlink&quot; title=&quot;关于迁移&quot;&gt;&lt;/a&gt;关于迁移&lt;/h2&gt;&lt;p&gt;博客总是在起起伏伏，关了又开，开了又关中反复，这一次，我将风向标博客放在了Github Page上。&lt;br&gt;程序采用了当下比
      
    
    </summary>
    
    
      <category term="ai0by" scheme="https://sbcoder.cn/tags/ai0by/"/>
    
  </entry>
  
  <entry>
    <title>岛国推特妹子图爬虫</title>
    <link href="https://sbcoder.cn/2019/03/22/Japan_Twitter_Spider.html"/>
    <id>https://sbcoder.cn/2019/03/22/Japan_Twitter_Spider.html</id>
    <published>2019-03-22T03:54:58.000Z</published>
    <updated>2019-03-22T05:11:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>LOC的大佬们分享了一个网站，收集了很多岛国的妹子图和她们的推特<br>地址：<a href="http://jigadori.fkoji.com" target="_blank" rel="noopener">岛国妹子推特</a><br>推特不是很感兴趣，就爬一下图片好了~</p><h2 id="爬虫介绍"><a href="#爬虫介绍" class="headerlink" title="爬虫介绍"></a>爬虫介绍</h2><h3 id="爬虫环境："><a href="#爬虫环境：" class="headerlink" title="爬虫环境："></a>爬虫环境：</h3><ul><li>Python2.7.9 可更替为3，自行更替</li><li>BeautifulSoup4</li><li>requests</li></ul><h3 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spy</span><span class="params">(url)</span>:</span></span><br><span class="line">    req = urllib2.Request(url)</span><br><span class="line">    req = urllib2.urlopen(req)</span><br><span class="line">    page = req.read()</span><br><span class="line">    soup = BeautifulSoup(page, <span class="string">"html.parser"</span>)</span><br><span class="line">    <span class="keyword">for</span> imgSoup <span class="keyword">in</span> soup.find_all(<span class="string">'div'</span>, &#123;<span class="string">"class"</span>: <span class="string">"row"</span>&#125;):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> imgSoup.find_all(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'photo'</span>&#125;):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> i.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'photo-link-outer'</span>&#125;).find(<span class="string">'a'</span>).find_all(<span class="string">'img'</span>):</span><br><span class="line">                img = j.get(<span class="string">"src"</span>)</span><br><span class="line">                <span class="keyword">print</span> img</span><br><span class="line">                str = random.sample(<span class="string">'zyxwvutsrqponmlkjihgfedcba'</span>, <span class="number">6</span>)</span><br><span class="line">                downImg(img, str)</span><br><span class="line">    nexturl = soup.find(<span class="string">'p'</span>,&#123;<span class="string">'class'</span>:<span class="string">'go-to-next-page'</span>&#125;)</span><br><span class="line">    nexturl = nexturl.find(<span class="string">'a'</span>).get(<span class="string">'href'</span>)</span><br><span class="line">    pageurl = <span class="string">"http://jigadori.fkoji.com"</span>+nexturl</span><br><span class="line">    spy(pageurl)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">downImg</span><span class="params">(img,m)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(img)</span><br><span class="line">    <span class="keyword">except</span> Exception , e:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"图片获取失败"</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./img/good%s.jpg'</span> % m, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(r.content)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url = <span class="string">"http://jigadori.fkoji.com"</span></span><br><span class="line">    spy(url)</span><br></pre></td></tr></table></figure><h2 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h2><p>看一下，网页构造，发现首页底部有下一页标签，BeautifulSoup取Class取值递归获取下一页地址<br>图片同上<br>整体难度不高，有兴趣的可以拿这个网站练练手~</p><h2 id="演示截图"><a href="#演示截图" class="headerlink" title="演示截图"></a>演示截图</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://fulicos.sbcoder.cn/2019/03/21/5c92e798d4689.png" alt="演示数据1" title>                </div>                <div class="image-caption">演示数据1</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://fulicos.sbcoder.cn/2019/03/21/5c92e794e9827.png" alt="演示数据2" title>                </div>                <div class="image-caption">演示数据2</div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;LOC的大佬们分享了一个网站，收集了很多岛国的妹子图和她们的推特&lt;br&gt;地址：&lt;a href=&quot;http://jigadori.fkoji.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;岛国妹子推特&lt;/a&gt;&lt;br&gt;推特不是很感兴趣，就爬一下图片好了
      
    
    </summary>
    
      <category term="Python" scheme="https://sbcoder.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="https://sbcoder.cn/tags/Python/"/>
    
      <category term="爬虫" scheme="https://sbcoder.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="福利" scheme="https://sbcoder.cn/tags/%E7%A6%8F%E5%88%A9/"/>
    
  </entry>
  
  <entry>
    <title>Python+selenium针对网银控件过登录取数据</title>
    <link href="https://sbcoder.cn/2019/03/21/Python_WY_Spider.html"/>
    <id>https://sbcoder.cn/2019/03/21/Python_WY_Spider.html</id>
    <published>2019-03-21T08:53:30.000Z</published>
    <updated>2019-03-22T03:10:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Python获取WY数据的方法总结："><a href="#Python获取WY数据的方法总结：" class="headerlink" title="Python获取WY数据的方法总结："></a>Python获取WY数据的方法总结：</h2><ul><li>由于WY控件加密方式相比较一般网站比较特殊，需在驱动层进行操作</li><li>由于WY控件的特殊性，获取交易数据的时候需要传递KEY (以CCB为例)</li><li>由于取数据时的问题，所有操作均应该在驱动层完成</li><li>要想实现实时更新数据，需要不断地登录，大部分WY都有强制退出操作</li></ul><h2 id="理清思路"><a href="#理清思路" class="headerlink" title="理清思路"></a>理清思路</h2><p>登录的过程中，由于安全控件的限制，需要绕过登录限制，此处思路借鉴了 <a href="https://blog.csdn.net/Bone_ACE/article/details/80765299" target="_blank" rel="noopener">爬虫应对银行安全控件</a>，由此可知，需要绕过登陆限制需从驱动层入手</p><p>两种方案</p><ul><li>Python可以使用 Win32api 模块来模拟键盘指令，类似于按键精灵的概念</li><li>Python使用 Photomjs 无界面浏览器配合Selenium Webdirver</li></ul><p>尝试使用Win32api时，由于需要配合鼠标操作，需要获取句柄坐标，且开发难度较高，尝试更换另一种方式<br>更换 Photomjs ，模拟登录时发现 Photomjs 并没有附带安全控件，所传输的值不会自动加密，尝试更换为 ChromDriver<br>使用 ChromDriver 尝试模拟登陆</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">username = browser.find_element_by_name(<span class="string">'USERID'</span>)</span><br><span class="line">username.send_keys(username)</span><br><span class="line">password = browser.find_element_by_name(<span class="string">'LOGPASS'</span>)</span><br><span class="line">password.send_keys(password)</span><br><span class="line">tjButton = browser.find_element_by_id(<span class="string">'loginButton'</span>)</span><br><span class="line">tjButton.click()</span><br></pre></td></tr></table></figure><p>登录成功！</p><p>当越过了登录后就需要获取交易信息，交易信息这一块，CCB的查询地址附带了一个SKEY，每次查询信息的时候都需要一个SKEY验证，如果不正确将不会返回正确的结果！<br>如何获取SKEY，涉及到WY的信息，这里不便细说（PS：细心地同学一定可以找到）</p><p>取到SKEY后即可构造查询地址，然后使用WebDriver模拟访问<br>需要注意的一点是，WY的大部分数据均是用iframe嵌套的，因此需要多处过iframe</p><p>演示代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定位到iframe</span></span><br><span class="line">iframe = browser.find_element_by_id(<span class="string">"iframe"</span>)</span><br><span class="line"><span class="comment"># 切换到iframe</span></span><br><span class="line">browser.switch_to_frame(iframe)</span><br></pre></td></tr></table></figure></p><p>取数据这一块不多说，每个WY也都不同<br>获取到数据后就是数据处理了，根据系统不同，我这里直接使用Python向固定地址POST传值</p><p>取数据后为了获取实时数据，需要定时向固定地址提交数据，大部分的WY都有长时间自动登出的骚操作，对此，思路也很多</p><p>大致几个想法</p><ul><li>自动刷新，保持登录状态</li><li>重复登录，更换SKEY，反复操作</li><li>模拟点击，保持登录状态</li></ul><p>自动刷新方案在一开始就失败了，多次频繁的刷新，会导致弹出手机验证码<br>重复登录，由于上次的失败，设置了延时，效果还可以，只是数据总会有延迟<br>模拟点击，无效，仍然会自动登出</p><p>采用重复登录的方式，递归实现！</p><h2 id="综上所述，我们可以将交易流程如此划分："><a href="#综上所述，我们可以将交易流程如此划分：" class="headerlink" title="综上所述，我们可以将交易流程如此划分："></a>综上所述，我们可以将交易流程如此划分：</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://fulicos.sbcoder.cn/2019/03/22/5c9438269d9c1.jpg" alt="WY数据.jpg" title>                </div>                <div class="image-caption">WY数据.jpg</div>            </figure><h2 id="成果演示"><a href="#成果演示" class="headerlink" title="成果演示"></a>成果演示</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://fulicos.sbcoder.cn/2019/03/22/5c944f127c9cc.jpg" alt="WY演示.jpg" title>                </div>                <div class="image-caption">WY演示.jpg</div>            </figure><p>Tips:本文仅做思路分享，切勿用在实际生产环境！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Python获取WY数据的方法总结：&quot;&gt;&lt;a href=&quot;#Python获取WY数据的方法总结：&quot; class=&quot;headerlink&quot; title=&quot;Python获取WY数据的方法总结：&quot;&gt;&lt;/a&gt;Python获取WY数据的方法总结：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="Python" scheme="https://sbcoder.cn/categories/Python/"/>
    
    
      <category term="Python" scheme="https://sbcoder.cn/tags/Python/"/>
    
      <category term="网银" scheme="https://sbcoder.cn/tags/%E7%BD%91%E9%93%B6/"/>
    
      <category term="selenium" scheme="https://sbcoder.cn/tags/selenium/"/>
    
      <category term="爬虫" scheme="https://sbcoder.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>
