{"meta":{"title":"风向标 | 分享与创造","subtitle":null,"description":null,"author":"ai0by","url":"https://sbcoder.cn","root":"/"},"pages":[{"title":"api","date":"2019-04-03T02:17:31.000Z","updated":"2019-04-04T07:33:08.000Z","comments":true,"path":"api/index.html","permalink":"https://sbcoder.cn/api/index.html","excerpt":"","text":"风向标API合集 名称 作用 接口 微博图床api 远程图片上传到微博图床 https://sbcoder.cn/api/sinaImg.html 生成二维码 将地址转换为二维码图片 https://sbcoder.cn/api/qrcode.html 更多api请持续关注…"},{"title":"api","date":"2019-04-04T07:27:21.000Z","updated":"2019-04-04T07:31:54.000Z","comments":true,"path":"api/qrcode.html","permalink":"https://sbcoder.cn/api/qrcode.html","excerpt":"","text":"二维码生成介绍使用场景很多，简单来讲，给我地址，我给你图，直接将地址放在img标签内即可~ 参数说明以GET的方式提交到：https://api.0161.org/qrcode/qrcode.php 参数 数值类型 示例 是否必传 url String https://sbcoder.cn 是 err String L (L,M,Q,H四种对应容错级别，不传默认L) 否 size String 7 (可以选择1~9999之间的值，对应不同大小，默认7) 否 logo String https://sbcoder.cn/img/avatar.jpg 否 返回值类型 : 直接返回图片 演示直接访问以下地址 12https://api.0161.org/qrcode/qrcode.php?url=https://sbcoder.cnhttps://api.0161.org/qrcode/qrcode.php?url=https://sbcoder.cn&amp;err=L&amp;size=7&amp;logo=https://sbcoder.cn/img/avatar.jpg 示例图片"},{"title":"api","date":"2019-04-04T07:27:21.000Z","updated":"2019-04-04T07:27:58.000Z","comments":true,"path":"api/sinaImg.html","permalink":"https://sbcoder.cn/api/sinaImg.html","excerpt":"","text":"微博图床-远程图片上传api介绍我们在使用爬虫相关内容的时候，存放图片时往往会遇到图片尺寸过大，存储不方便等问题，这时候，存放在一个永久存储的云上面就很有必要，微博是一个不限流量，全球CDN的图床~微博也是有缺点的，他并不是一个易于管理的图床，仅限于存放图片但不能管理图片，如果希望使用可以管理的图床，可以参考使用自建图床，参考我的:FuliCOSIMG 参数说明以GET的方式提交到：https://api.0161.org/sinaimg/sinaImg.php 传递参数类型：GET POST 参数 数值类型 示例 是否必传 url String https://sbcoder.cn/img/avatar.jpg 是 返回参数类型 ： JSON 演示示例:1&#123;\"large\":\"http://ww2.sinaimg.cn/bmiddle/0062WdSely1g1p8mlciyrg30390120jl.gif\"&#125; PHP DEMO12345678910111213141516171819$data = array( 'url' =&gt; \"https://sbcoder.cn/img/avatar.jpg\", );echo curlPost(\"https://api.0161.org/sinaimg/sinaImg.php\",$data);function curlPost($url,$res)&#123; $curl = curl_init(); curl_setopt($curl, CURLOPT_URL, $url); curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, 0); curl_setopt($curl, CURLOPT_FOLLOWLOCATION, 1); curl_setopt($curl, CURLOPT_AUTOREFERER, 1); curl_setopt($curl, CURLOPT_POST, 1); curl_setopt($curl, CURLOPT_POSTFIELDS, http_build_query($res)); curl_setopt($curl, CURLOPT_TIMEOUT, 30); curl_setopt($curl, CURLOPT_HEADER, 0); curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1); $result = curl_exec($curl); curl_close($curl); return $result;&#125;"},{"title":"categories","date":"2019-03-21T07:11:04.000Z","updated":"2019-03-21T07:11:32.000Z","comments":false,"path":"categories/index.html","permalink":"https://sbcoder.cn/categories/index.html","excerpt":"","text":""},{"title":"Links","date":"2019-04-22T12:14:24.000Z","updated":"2019-06-20T10:45:45.000Z","comments":true,"path":"custom/index.html","permalink":"https://sbcoder.cn/custom/index.html","excerpt":"","text":"我的朋友 - 排名不分先后 - OwCrypto加密货币百科 - Huas Leung’s Blog - whan的博客 申请友情链接 直接在下面留言即可！ 要求： - 正规站点 - 博客类优先 - 技术类站点优先 本站链接格式 - 风向标博客 - https://sbcoder.cn"},{"title":"tags","date":"2019-03-21T05:14:24.000Z","updated":"2019-03-21T05:14:52.000Z","comments":false,"path":"tags/index.html","permalink":"https://sbcoder.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"公司代码架构 - Docker + Jenkins + Gogs + Portainer(二)","slug":"公司代码架构-Docker-Jenkins-Gogs-Portainer-二","date":"2019-12-10T11:05:36.000Z","updated":"2019-12-10T11:07:02.000Z","comments":true,"path":"2019/12/10/gogs_docker.html","link":"","permalink":"https://sbcoder.cn/2019/12/10/gogs_docker.html","excerpt":"","text":"安装 Gogs + Docker常用命令介绍本节主要写一下Jenkins的配置与自动构建过程，包括使用Gogs作为git服务器，配置自动构建等。本节需要配合上一节的内容使用，即 安装 Docker + Jenkins 的服务器一台 搭建环境 服务器3 ：TencentCloud 北京 RAM4G 2C 40GSSD（新用户机器）998RMB/3Year 操作系统 ：CentOS7.5 部署环境 ：LNMP1.6 （我是军哥铁粉） 服务器1：搭建Jenkins中转服务器，做代码自动构建使用服务器2：生产环境服务器，实则测试服务器，部署代码使用服务器3：Gogs服务器，Git版本库服务器，做代码版本控制使用 配置无需对标，都是低配置小鸡，唯一一个腾讯云 2C4G 的机器是我之前放其他业务的机器，由于git经常需要使用因此搭建在国内套CloudFlare使用，实则Gogs只需要 2C1G 机器即可，官方推荐配置是2C512M，是一个不吃内存的程序，目前腾讯云的 1C2G 只需要99/年，属于大众所承受的起的价格，由于我不是专职AFFMAN，因此不贴链接 CentOS7 安装 Docker1yum -y install docker 1.png 12start dockerenable docker 2.png 查看版本号，查看是否安装成功！1docker -v 3.png Docker 常用命令记录一下安装时可能会出现的问题，以及常用的Docker命令 查看当前运行的容器1docker ps 查看所有容器1docker ps -a 停止容器12docker stop [DOCKER NAME] # 例如：docker stop gogs 删除容器(必须在Stop之后才可以删除)12docker rm [DOCKER NAME] # 例如：docker rm gogs 进入容器1234docker attach [DOCKER NAME] # 例如： docker attach gogsdocker exec -it [DOCKER IMAGE ID] /bin/bash# 例如： docker exec -it ef5cb0692b57 /bin/bash 退出容器1exit 查看容器变动日志12docker diff [DOCKER NAME]# 例如：docker diff gogs 查看容器或者镜像详细信息12sudo docker inspect [IMAGE NAME]:0.1 # 例如： sudo docker inspect gogs 向容器内部发送指令12docker exec [DOCKER NAME] [COMMAND]# 例如 docker exec gogs ls 安装 Gogs安装下载镜像 Gogs1docker pull gogs/gogs 4.png 创建目录12mkdir -p /home/Gogscd /home/Gogs 5.png 开启Docker1234# 直接启动docker run --name=gogs -p 10022:22 -p 10080:3000 -v /home/Gogs:/data gogs/gogs# 后台启动docker run --name=gogs -d -p 10022:22 -p 10080:3000 -v /home/Gogs:/data gogs/gogs 配置正常启动后直接打开 http://ip:10080即可 如图： 6.png 接下来按照提示配置即可，我们这里使用SQLite3 应用配置需要注意一下 域名填写你的服务器公网IP SSH端口号填写 映射的端口号 10022 HTTP端口号填写 3000 应用URL填写 ip + 映射的端口号 10080 访问 7.png 配置完成后如果设置好管理员账户的会自动登录进去，如果没有设置的可以自行注册。 注意：数据库中第一个用户就是管理员账户 8.png Tips：Gogs的配置文件存放在 Docker中的 /data/gogs/conf/app.ini 如果想要更改可以 反向代理绑定域名同 公司代码架构 - Docker + Jenkins + Gogs + Portainer(一)一样，在应用配置阶段修改或者修改配置文件都可以 结语Gogs 是一个轻量级的 Git服务器，适用于一些小公司小团队使用，大公司使用Gitlab的情况可能更多一些，但是东西实际都是差不多的，为了占用资源更小一些，我这里还是选用了比较轻量级的Gogs","categories":[{"name":"架构","slug":"架构","permalink":"https://sbcoder.cn/categories/架构/"}],"tags":[{"name":"版本控制","slug":"版本控制","permalink":"https://sbcoder.cn/tags/版本控制/"},{"name":"优化","slug":"优化","permalink":"https://sbcoder.cn/tags/优化/"},{"name":"架构","slug":"架构","permalink":"https://sbcoder.cn/tags/架构/"}]},{"title":"公司代码架构 - Docker + Jenkins + Gogs + Portainer(一)","slug":"公司代码架构 - Docker + Jenkins + Gogs + Portainer(一)","date":"2019-12-09T11:47:26.000Z","updated":"2019-12-10T11:07:37.000Z","comments":true,"path":"2019/12/09/code_build.html","link":"","permalink":"https://sbcoder.cn/2019/12/09/code_build.html","excerpt":"","text":"公司代码架构 - Docker + Jenkins + Gogs + Portainer(一)介绍公司研发项目时，遇到git作为版本控制时，很常见的问题是部署比较麻烦（相比较麻烦），需要先克隆在拉到服务器部署，代码提交频率过高时，就会出现一天很多次提交代码，部署代码，浪费了大量的人力物力，于是乎大量的架构师技术总监们开始研究各类解决方案，各种上线前review代码，这是一件非常痛苦的事情，这里简单记录一下公司代码架构的部署，来解决公司代码架构上的诸多问题，也是自己做一个笔记，文章内如有错误，还请各位指出。 Tips：该环境并不适用于个人用户以及小微企业，适用于项目众多且开发人员大于30人的公司 搭建环境 服务器1 ：Virmach 水牛城 RAM1.8G 2C 10GSSD（黑五机器） 操作系统 ：Ubuntu16.04 部署环境 ：LNMP1.6 （我是军哥铁粉） 这里说明一下，多台服务器是为了解耦，说是解耦，实则认为承受不住，且大部分公司已经有一套完整的git服务器了，本节要做的就是加个自动构建而已，git服务器可自选环境，后面会讲如何搭建Gogs，如果有高配置服务器的公司或个人，可以尝试使用单服务器多部署，本文所需共三台服务器。 服务器1：搭建Jenkins中转服务器，做代码自动构建使用服务器2：生产环境服务器，实则测试服务器，部署代码使用服务器3：Gogs服务器，Git版本库服务器，做代码版本控制使用 服务器1的配置属于中下配置，该机型一年 13刀，属于大部分人都承受的起的价格，请注意，本文标注的配置仅用于配置自动构建服务器，并不用于部署代码以及Git服务，个人用户小鸡多的可以尝试 Ubuntu16.04/Ubuntu18.04 安装 DockerDocker可以说是非常的牛X了，关于他的概念不多说，牛就牛在管理太方便了，就好像是 WHMCS 用母鸡开小鸡一样，母鸡永远不考虑小鸡的运作，只需要配合就好，Docker也一样，我们只需要创建容器，管理容器就够了，剩下的交给Docker来处理，且Docker可以做负载均衡，后续做架构的时候可以开多个Docker做集群，按需来做 安装删除旧版本，更新apt-get，安装docker123sudo apt-get remove docker docker-engine docker-ce docker.iosudo apt-get updatesudo apt install docker.io 启动Docker并查看版本启动docker123systemctl start dockersystemctl enable dockerdocker --version 我这里安装的是 18.09版本 Docker1.png 安装Jenkins安装安装Jenkins可以选择多种方式安装，我这里采用的是Docker的方式安装的，由于我们上面已经安装过Docker，按照Jenkins官方给的安装方案就可以了，首先pull一个稳定版本的 Jenkins 镜像镜像地址 : jenkinsci/blueocean12docker pull jenkinsci/blueoceandocker images Docker2.png 查看当前Jenkins版本1docker inspect [IMAGE ID] 这里的 IMAGE ID 则是上面 查看镜像的 IMAGE ID Jenkins1.png 红框内则是 对应版本号 123456# 创建一个存放Jenkins Docker的目录mkdir /home/root/Jenkins# 启动一个Dockerdocker run -d --name jenkins -p 8081:8080 -v /home/jenkins:/home/jenkins jenkins/jenkins:lts# 查看jenkins服务docker ps | grep jenkins Jenkins2.png 打开 http://IP:8081 Jenkins3.png 输入命令进入 Docker内部123docker exec -it jenkins bash# 获取密码cat /var/jenkins_home/secrets/initialAdminPassword 重启Docker1docker restart [CONTAINER ID] [CONTAINER ID] 为当前Jenkins的版本号 例如 2.164.3 则输入 docker restart 2.164.3 Nginx反代Jenkins，绑定域名由于众所周知的水牛城服务器卡，国内环境必须搭配CloudFlare才可以使用，否则太卡了，因此安装LNMP环境，后续自动构建时也可以构建到这里 安装方法一： LNMP环境安装教程 : LNMP一键安装包 - 安装教程 懒得打开的话，可以直接输入1wget http://soft.vpser.net/lnmp/lnmp1.6.tar.gz -cO lnmp1.6.tar.gz &amp;&amp; tar zxf lnmp1.6.tar.gz &amp;&amp; cd lnmp1.6 &amp;&amp; ./install.sh lnmp 我这里安装的是 1.6稳定版，需要其他版本的可以自行修改版本号安装 安装方法二： apt-get安装，建议先更新apt-get1apt-get install nginx 打开NGINX配置文件目录，创建一个新的配置文件(这里是lnmp环境的配置文件地址，apt-get安装的nginx配置文件存放于 /etc/nginx/conf.d 中) 1vi /usr/local/nginx/conf/vhost/jenkins.conf 输入以下内容，请注意替换掉下面的 jenkins.0161.org1234567891011121314server &#123; listen 80; server_name jenkins.0161.org; client_max_body_size 60M; client_body_buffer_size 512k; location / &#123; proxy_pass http://localhost:8081; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 重启lnmp使配置生效1lnmp reload 打开CloudFlare绑定域名，增加CDN支持，此处比较简单，不多赘述 结语一个完整的架构，需要很多的付出，并非一朝一夕，本文所属架构并不适用于所有公司或者个人，大型公司还需要K8s集群，承受多少并发取决于很多因素，任何架构都不能通杀所有类型的公司","categories":[{"name":"架构","slug":"架构","permalink":"https://sbcoder.cn/categories/架构/"}],"tags":[{"name":"版本控制","slug":"版本控制","permalink":"https://sbcoder.cn/tags/版本控制/"},{"name":"优化","slug":"优化","permalink":"https://sbcoder.cn/tags/优化/"},{"name":"架构","slug":"架构","permalink":"https://sbcoder.cn/tags/架构/"}]},{"title":"开发中常见的MySQL数据库优化细节","slug":"开发中常见的MySQL数据库优化细节","date":"2019-06-20T10:17:03.000Z","updated":"2019-12-09T11:51:26.000Z","comments":true,"path":"2019/06/20/mysql_optimize.html","link":"","permalink":"https://sbcoder.cn/2019/06/20/mysql_optimize.html","excerpt":"","text":"前言以我的习惯来讲，每开始一个新的项目都需要先把思路完善，紧接着就需要建立数据库，在码代码的时候，就一般不会在修改数据库的构造了，因此，数据库的结构通常关乎着查询的速度以及程序的完善程度，一个好的结构可以让你少写很多代码，也能让程序的运行速度更加快，通常在大公司都是由DBA来做这件事，但是事无绝对，作为一名合格的后端，掌握一些少量的数据库优化也是很需要的。 MySQL优化 - 数据类型及CURDPROCEDURE ANALYSE()PROCEDURE ANALYSE() [prəˈsējər ˈænəlaɪz]是一个MySQL自带的给我们提供数据库优化建议的函数，他可以直接运行在MySQL中，直接在执行语句中加上这个函数即可1SELECT * FROM `list` WHERE 1 PROCEDURE ANALYSE ( ) 这段SQL执行过后，将会把list表中的数据分析一遍，并把他的分析结果展示出来 Field_nameMin_valueMax_valueMin_lengthMax_lengthEmpties_or_zerosNullsAvg_value_oravg_lengthstdOptimal_fieldtype 他将会把分析出来的 字段名 最短值 最大值 以及最后一列就是MySQL给出的分析结果，我们可以在有一定数据的时候使用这个函数来分析，这样给出的结果会更精确一些，只需要查看最后一列Optimal_fieldtype的值即可，这个函数并不适用于数据库设计阶段，它适用于后期使用 EXPLAINEXPLAIN是一个非常好用的MySQL语法，在我们功能测试阶段，如果发现某页面非常慢，排除静态资源问题后就可以试试使用EXPLAIN，我们可以在执行语句前面加上 EXPLAIN 来获得执行过程，通过该结果我们可以看到SQL如何改变会减少查询时间和次数。1EXPLAIN SELECT * FROM `list` WHERE 1 这段SQL执行后，将会返回如下格式的分析结果 idselect_typetabletypepossible_keyskeykey_lenrefrowsExtra 我们主要看rows就行，为了得到想要的结果，rows的值越小越好，使用EXPLAIN来调试简直是再好不过了！ ENUM(枚举)类型很多程序员往往喜欢统一一个数据类型，比如说 ‘varchar’ ，这可能是我见过最多的数据类型了，早些时期，的确是有很多的公司或者程序都是大面积使用，随着MySQL的革新换代，很多的类型都可以避免使用它。我在很多得程序上测试过（有数据）PROCEDURE ANALYSE()方法，他给出了很多 ‘varchar’ 替换为 ‘enum’ 的建议，这说明，enum类型的确是一个应该被重视的数据类型，但由于他是一个枚举类型，我们在定义数据类型的时候并不适合直接上手定义，所以很多时候都是在有一定的数据量的时候才想要换数据类型的。可以理解为枚举即时索引，枚举就相当于给这个字段的可能值都加上了一个索引，与我们为了优化查询加索引是一样的概念。enum更适用于选项卡类字段，例如性别，订单状态等，如果您字段中只有几个重复的值也是非常推荐使用的。 JOIN链接查询，这是我们在开发中非常常用的查询方式，首先要知道，我们在学校里学习的大多数是 AND 链接多表查询，虽然能够将结果无误的查询出来，但是速度就影响的非常多了，这里还是推荐大家使用JOIN来连接查询有些同学可能不太理解JOIN，简单说一下JOIN的内连接和外链接，左外链接和右外链接吧 内连接即是A B两表链接，只取两表共有的数据，假设 B 中 有的数据 A 表内没有对应的数据则无法查询到 1SELECT * FROM list1 INNER JOIN list2 on list1.id = list2.id 外连接（FULL JOIN 也称作全连接）即是A B两表链接，取两表所有的数据，即使 B 表中的某些数据无法匹配链接条件时，也正常链接 1SELECT * FROM list1 FULL JOIN list2 on list1.id = list2.id 左外连接，即是 A B两表链接，取两表所有数据，若A表中有B表不匹配的数据，同样展示出来，B表如果有A不匹配的数据，则不展示 1SELECT * FROM list1 LEFT OUTER JOIN list2 on list1.id = list2.id 右外连接，即是 A B两表链接，取两表所有数据，若B表中有A表不匹配的数据，同样展示出来，A表如果有B不匹配的数据，则不展示，与左外连接相反 VvmQFU.png MySQL优化 - 结构FULLTEXT INDEXFULLTEXT INDEX(全文索引)，更适用于文章内容搜索的索引，我们在作搜索功能的时候，很多人喜欢将文章内容(content)建立普通索引，但是实际上，这种做法并不会增加查询速度，通常我们做搜索的时候，执行下列语句。 1SELECT content FROM `list` WHERE content LIKE '%风向标%' 如果搜索功能权重比较高的网站，就需要将content这个字段建立索引。 1ALTER TABLE `list` ADD FULLTEXT (`content`) 如果是phpmyadmin用户，在phpmyadmin中直接点击’全文搜索’即可。 MyISAM OR InnoDB？就我现阶段写出来的东西来看（数据量小，查询次数少，用户量较少），MyISAM肯定是最适合我的，它更适用于小型网站，以及事务处理较少的网站InnoDB则与之相反，如果你的业务比较复杂，针对数据库的操作较多的时候，InnoDB就会更适合一些。使用INSERT插入数据时 MyISAM 就比 InnoDB 更快一些，而 UPDATE 时 InnoDB 就会比 MyISAM 快一些 如果您是轻度SQL用户，重功能不重视业务的项目，那么我个人以为 MyISAM 更适合一些如果您感觉业务逻辑复杂，经常使用SQL，那么可以尝试使用 InnoDB 最后也是见仁见智，没有好坏，如果您希望测试，也是可以通过直接修改数据库引擎来测试速度的 MySQL优化 - 小知识点 不要使用 SELECT * 查询 不要使用 NULL 频繁查询的字段建立索引 索引过多时会影响 UPDATE 和 INSERT 的执行速度 避免在 WHERE 时使用 != &lt;&gt; 等操作符，MySQL会自动放弃索引，直接全表扫描 避免使用 IN 和 NOT IN，尽量使用BETWEEN，MySQL会自动放弃索引，直接全表扫描 可以使用 EXISTS 来代替 IN 使用 某些情况下可以使用强制使用索引查询 SELECT * FROM list with(index(索引名)) WHERE …. 避免使用 OR 作为调件，可以使用 UNION 并集查询将两次查询结果合并 尽可能将表内容长度固定 查询时如果只查询一条信息，就使用 LIMIT 1 避免使用比较表达式 如 10000+1 = id 可以使用 id = 10000+1 记得将查询链接即时关闭掉 使用变量来给MySQL开启查询缓存，避免使用MySQL内置变量函数 设置的主键尽量使用长度短且最好是int类型 垂直分割，将大量的字段的表优化成多个少字段的表 INSERT 和 DELETE 是一个可以锁定数据表的SQL语句，必须等待执行完毕后才会解除锁定，如果这条语句执行起来过于缓慢，请谨慎使用 Object Relational Mapper Prepared Statements 参考: Top 20+ MySQL Best Practices参考: 廖雪峰的个人网站 - 链接查询","categories":[{"name":"优化","slug":"优化","permalink":"https://sbcoder.cn/categories/优化/"}],"tags":[{"name":"优化","slug":"优化","permalink":"https://sbcoder.cn/tags/优化/"},{"name":"MySQL","slug":"MySQL","permalink":"https://sbcoder.cn/tags/MySQL/"}]},{"title":"mm131全栈多线程爬虫","slug":"mm131全栈多线程爬虫","date":"2019-06-19T14:34:31.000Z","updated":"2019-06-19T14:56:33.000Z","comments":true,"path":"2019/06/19/mm131_spider.html","link":"","permalink":"https://sbcoder.cn/2019/06/19/mm131_spider.html","excerpt":"","text":"起因LOC的大佬们最近开始疯狂的爬取mm131，作为一个Python初心者，作为技术上的学习，也要与时俱进，简单写了一个图片下载爬虫，看到大佬们似乎是做了一个typechoo的对接接口，我这边回头有空也搞一个wordpress的接口（只在博客内发布），之前写过一个新浪远程上传的接口，由于种种原因，新浪已经不支持外链了，因此这个wordpress接口可能就有时间再做了，不然做出来也是个摆设，没地方放。 代码解析网址不发了，直接讲，或者大家直接百度谷歌都可以搜得到。打开网站,总共有如下六个分类 mm131.jpg 每个分类下面都有一堆的图集，有N个分页的图集，但是第一页跟第二页的地址还不太一样，这点跟192tt做的很相似，感觉这几个站长是不是都是用的同一套程序，如果是的话可以通杀了。。。首先遍历图集的地址，到目前更新文章截止，一共大概5000多套按分类给他搞一个分类循环 1234list = &#123;'xinggan':6,'qingchun':1,'xiaohua':2,'chemo':3,'qipao':4,'mingxing':5&#125; # list = &#123;'mingxing':5&#125; for key in list: getPageUrl(key,list[key]) 解析图片url，用正则获取就行，用bs4取到末页的地址，然后遍历循环取图集地址 1234567try: i.find('img').get('src')except Exception as e: for s in i.find_all('a'): endPage = s.get('href') endPage = rex('list_%s_(\\d+).html'%num,endPage) continue 获取图集地址12345678910nextUrl = \"%s/list_%s_%s.html\"%(url,num,i+2)response = requests.get(nextUrl, headers=headers)response.encoding = 'gb2312'soup = BeautifulSoup(response.text, 'html.parser')for i in soup.find('dl', &#123;'class': 'public-box'&#125;).find_all('dd'): try: i.find('img').get('src') except Exception as e: continue print(i.find('a').get('href')) 需要注意的是，mm131的图片是有防盗链的，根据referer判断，随便找一个图集的地址设置上即可1234headers = &#123; \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\", 'referer': \"http://www.mm131.com/xinggan/4995.html\",&#125; 线程池应用之前的爬虫除了Scrapy搞出来的之外都是单线程的爬虫，优点是比较稳定，但是缺点也很明显，太慢了，mm131这个站的图大都比较小，如果是一张一张下载确实是不太划算，于是搞了个线程池。Python的线程池很简单，只需要引入 threadpool 即可，如果报错，请 pip install threadpool,12345678910# 引入threadpoolimport threadpool# 创建线程池，设置为12线程，可以根据自身情况修改pool = threadpool.ThreadPool(12)# 创建callback函数，参数1 getSingleData 是需要调用的函数名，list是函数getSingleData的参数，该方法适用于单个参数的函数，list是一个一维数组或对象pageTask = threadpool.makeRequests(getSingleData, list)# 执行线程池[pool.putRequest(req) for req in pageTask]# 等待完成后退出pool.wait() 演示和下载演示图 多线程演示.jpg 下载结果.jpg 下载演示代码不全，直接上地址https://github.com/ai0by/ai0by-spider/tree/master/mm131","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"福利","slug":"福利","permalink":"https://sbcoder.cn/tags/福利/"}]},{"title":"Redis/Redis集群以及在Laravel中的使用方法","slug":"Redis-Redis集群以及在Laravel中的使用方法","date":"2019-06-17T13:19:37.000Z","updated":"2019-06-17T13:21:08.000Z","comments":true,"path":"2019/06/17/Redis_laravel_PHP.html","link":"","permalink":"https://sbcoder.cn/2019/06/17/Redis_laravel_PHP.html","excerpt":"","text":"Redis的数据类型Redis也算是一种数据的容器，承载在内存上，因此它的各方面性能都比较快，且作为非关系型数据库，面对各种索引也比普通的数据库查询快，不同的场景下使用不同的数据类型，适用于很多地方 字符串类型 String一一对应，使用场景比较多 key:value 形式 命令 描述 set key value 设置指定key值 get key 获取指定key的value值 mget key1 key2 获取多个key 的 value，按顺序返回value值 mset key1 ‘value1’ key2 ‘value2’ 批量设置多个key的value strlen key 返回对应value长度 getrange key start end 截取字符串 append key value 追加key关联的value值，返回长度 getset key value 设置key的value并返回原value值 setex key time value 设置value值，并加上一个过期时间，使用ttl key查看过期时间，秒为单位 setnx key value 当key不存在时，设置value msetnx key1 ‘value1’ key2 ‘value2’ 当所有的key都不存在时，批量设置多个key的value incr key 将key关联value的值加一，仅对数字有效 incrby key num 将key关联value的值加num，例如 10，仅对数字有效 incrbyflout key num 将key关联value的值加num，浮点类型 decr key 将key关联value的值减一，仅对数字有效 decrby key num 将key关联value的值减num，例如 10，仅对数字有效 哈希类型 Hash一对一对多，类似字符串，但又区别于字符串，它比字符串复杂一些，同样是key:value，但是他的value可以是一个map，同时，它也无法给单个属性赋予过期时间，但可以给单个属性设置值，某些情况下比String占用资源少，当需要缓存整张表时推荐使用 命令 描述 hset key field value 设置key关联的value hkeys key 获取所有的key hgetall key 获取key的所有对应field hvals key 获取hash表中所有的value hlen key 获取keyd的长度 hmget key field1 field2 获取多个field的值 hmset key field1 value1 field2 value2 设置多个field的值 hdel key field 删除单个field的单个属性 hsetnx key field value 当field不存在时存储数值 hincrby key field num 给指定字段增加数值，整数 hincrbyfloat key field num 给指定字段增加浮点数 列表类型 List类似栈，拥有栈的特性，也有链表的特性，亦可用作消息队列等场景，使用场景很广 命令 描述 lpush key value1 value2 将多个value插入到关联的key里面 头部 lpushx key value 将value插入到key中，需要key已经存在 头部 lpop key 删除并获取当前key里面的第一个元素 llen key 获取当前key关联的list长度 rpush key value1 value2 将多个value插入到关联的key里面 尾部 rpop key 删除并获取列表内的最后一个元素 rpushx key value 将value插入到key中，需要key已经存在 尾部 blpop key1 key2 timeout 删除并获取列表的第一个元素，key1存在时不执行key2，如果没有会一直阻塞到弹出为止，建议添加延时 brpop key1 key2 timeout 删除并获取列表的最后一个元素，key1存在时不执行key2，如果没有会一直阻塞到弹出为止，建议添加延时 lindex key 通过索引来获取list中的元素 lset key index value 通过索引来设置相应元素的值 lrange key start end 截取指定列表内元素 ltrim key start end 只保留开始和结束内的元素 集合 set数据池，无序，可计算差集交集等，之前写爬虫时用集合做过去重，Python使用redis也是非常方便的 命令 描述 sadd key member1 member2 向集合内添加元素 scard key 获取集合内元素数量 smembers key 获取集合内所有的元素 sismember key member 判断member是否是key集合的子元素 sdiff key1 key2 获取给定集合的差集 sinter key1 key2 获取给定集合的交集 sunion key1 key2 获取给定集合的并集 spop key 随机删除一个集合内元素并返回 srandmember key num 返回集合内的一个或者多个随机元素 srem key member1 member2 删除集合中一个或者多个指定元素 其他剩下的数据类型确实是没用过，这里不便多说 Laravel使用redis流程简单来说如下图所示 Laravel使用redis 程序将数据存储请求发送给Laravel内置的redis模块（PHPRedis，Predis等），并在config/database.php中配置好redis的端口密码等信息，通过内置模块调用已经安装好的redis即可使用redis存储使用数据了，然后redis内部处理数据我们如果不做底层的话，正常存储使用，只需要处理好程序与Laravel之间的过程就可以了，也就是说，了解PHPRedis和Predis就可以了，目前似乎大多数人使用的都是这两种，也不仅限于Laravel，原生PHP以及像Swoole这种的也是可以使用的。 关于Laravel中的Redis配置使用 可以参考 Laravel中文文档5.8 - redis 1234567// laravel 简单调用示例use Illuminate\\Support\\Facades\\Redis;class testRedis()&#123; Redis::set('username','风向标'); $username = Redis::get('username'); return $username;&#125; Laravel使用Redis集群仍然是在 config/database 中配置 clusters123456789101112131415161718192021222324252627282930313233'redis' =&gt; [ 'client' =&gt; env('REDIS_CLIENT', 'predis'), 'options' =&gt; [ 'cluster' =&gt; env('REDIS_CLUSTER', 'predis'), // 'cluster' =&gt; env('redis'), ], 'clusters' =&gt; [ 'vaneCache' =&gt; [ [ 'host' =&gt; env('REDIS_HOST', '127.0.0.1'), 'password' =&gt; env('REDIS_PASSWORD', null), 'port' =&gt; env('REDIS_PORT', 6379), 'database' =&gt; 1, ], [ 'host' =&gt; env('REDIS_HOST', '127.0.0.1'), 'password' =&gt; env('REDIS_PASSWORD', null), 'port' =&gt; env('REDIS_PORT', 6379), 'database' =&gt; 2, ], [ 'host' =&gt; env('REDIS_HOST', '127.0.0.1'), 'password' =&gt; env('REDIS_PASSWORD', null), 'port' =&gt; env('REDIS_PORT', 6379), 'database' =&gt; 3, ], ], ], ], 在使用时仅需要 使用 connection 即可 1234$redis1 = Redis::connection('vaneCache');$redis1-&gt;set('username','风向标');$username = $redis1-&gt;get('username');echo $username;","categories":[{"name":"PHP","slug":"PHP","permalink":"https://sbcoder.cn/categories/PHP/"}],"tags":[{"name":"优化","slug":"优化","permalink":"https://sbcoder.cn/tags/优化/"},{"name":"PHP","slug":"PHP","permalink":"https://sbcoder.cn/tags/PHP/"},{"name":"Laravel","slug":"Laravel","permalink":"https://sbcoder.cn/tags/Laravel/"},{"name":"Redis","slug":"Redis","permalink":"https://sbcoder.cn/tags/Redis/"}]},{"title":"tuwan（兔玩）全站妹子图爬虫可多窗口","slug":"tuwan全站妹子图爬虫可多窗口","date":"2019-05-13T15:01:00.000Z","updated":"2019-05-13T15:29:23.000Z","comments":true,"path":"2019/05/13/tuwan_spider.html","link":"","permalink":"https://sbcoder.cn/2019/05/13/tuwan_spider.html","excerpt":"","text":"前言兔玩是一个非常不错的妹子图网站，跟曾经的PR社有异曲同工之处，花少量的钱可以看Coser的图片，但是tuwan的妹子还是很正经的哟~兔玩官网 tuwanjun.com以下是网站截图 tuwan 兔玩的更新速度还是不错的呢~ 破解看到官网的套图打开后，一般是有几张可以看得图，也有一堆尺寸小的预览图，地址很相似，例如这张1http://img4.tuwandata.com/v3/thumb/jpg/YjAzYiwxNTgsMTU4LDksMywxLC0xLE5PTkUsLCw5MA==/u/GLDM9lMIBglnFv7YKftLBuvzpwYbEIoBh8ap84BsgXdniTdx80UqsXLdP5yaJZklUj09PvGO8IYpAC3nOanE0EHpB9bCnRKUnvdbAJH6CcXC.jpg YjAzYiwxNTgsMTU4LDksMywxLC0xLE5PTkUsLCw5MA==这一串很容易看的出是base64加密过的串，经过解密获得12base64.b64decode(imgurl)# b03b,158,158,9,3,1,-1,NONE,,,90 这里的158,158就是缩略图的尺寸了，我们尝试修改缩略图尺寸然后在base64加密后就可以取得地址，经过尝试，修改为 0，0即可还原原图尺寸~12base64.b64encode(base64.b64decode(imgurl.encode('utf-8')).replace('158','0'))# YjAzYiwwLDAsOSwzLDEsLTEsTk9ORSwsLDkw 组合成原图地址：1http://img4.tuwandata.com/v3/thumb/jpg/YjAzYiwwLDAsOSwzLDEsLTEsTk9ORSwsLDkw/u/GLDM9lMIBglnFv7YKftLBuvzpwYbEIoBh8ap84BsgXdniTdx80UqsXLdP5yaJZklUj09PvGO8IYpAC3nOanE0EHpB9bCnRKUnvdbAJH6CcXC.jpg 下载源代码已经开源到Github上了上一张测试图 测试图 下载地址：tuwan_spider","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"福利","slug":"福利","permalink":"https://sbcoder.cn/tags/福利/"}]},{"title":"Discuz会员数据与Wordpress互通","slug":"Discuz会员数据与Wordpress互通","date":"2019-04-11T12:06:06.000Z","updated":"2019-05-13T14:57:58.000Z","comments":true,"path":"2019/04/11/Discuz-Userinfo-To-Wordpress.html","link":"","permalink":"https://sbcoder.cn/2019/04/11/Discuz-Userinfo-To-Wordpress.html","excerpt":"","text":"情景这个情景可能遇到的也不在少数，不想舍弃用户数据，还想让用户无需注册在新站保留账号。实际当我们在迁移的时候，稍微了解数据库的同学应该明白想要迁移用户数据只需要迁移用户数据表即可。实际上我也是这么做的，但是中途遇到了几个小问题，这里我总结一下！ Discuz用户密码加密算法Discuz的用户信息都存放在 ‘pre_common_member‘ 表里，包含了我们需要转移的 邮箱,用户名,密码,积分,ip 等各类信息那么很简单了，便利这个表再插入到Wordpress表内即可但在导入表之前需要先测试一下用户数据是否匹配以示严谨~当我测试密码匹配的时候发现，这里的密码似乎并不匹配，首先我想到的就是应该是加盐了，但是纵观整个 ‘pre_common_member‘ 表，似乎并没有该有的字段网上找了一圈发现Discuz的用户真实密码是存在 ‘pre_ucenter_members‘ 表内的，’pre_common_member‘ 表内的密码我现在还不知道有什么用处，但至少跟我们需要迁移的数据没什么关联。从 ‘pre_ucenter_members‘ 表中找到了我们需要的 ‘salt‘ 字段，经过测试得出Discuz的密码加密算法为1md5(md5('password').'salt'); tips: Discuz里面的salt是一个6位的 数字+字母 随机数 Wordpress用户密码加密算法搞定了DZ的加密算法后，那么如何将DZ的用户信息插入到WP里面就很重要了，打开 Wordpress 的数据库找到 ‘wp-user‘表，找到 ‘user_pass‘ 字段，发现里面加密的内容似乎无迹可寻。实际上，Wordpress的加密是使用了 phpass 类来加密的，由 phpass 加密的密码具有不可逆性，所以想要破解是不可能了，这里简单说一下 phpass 的加密算法目前我们的PHP版本应该都在5以上，所以前缀是一样的 $P$B 大致写出来如下：12345$count = rand(1,8);$hash = md5($salt . $password, TRUE);while($count--)&#123; $hash = md5($hash . $password, TRUE);&#125; 看起来是不是很强，我们无法破解这样的密码，实际使用中，我们也可以使用 phpass 来做密码加密，让我们的数据库更加的安全~然而我们数据迁移时其实完全可以避免这种问题，Wordpress是保留了md5加密的形式的，如果 ‘user_pass‘ 字段里面存储的是md5加密的32值，wordpress也可以登录成功，并且再登陆后会将 ‘user_pass‘ 字段修改为 phpass 加密的格式，是不是很人性化呢。综上所述，我们无需解出来wordpress的加密算法，我们也解不出来~ 开始迁移用户数据关于迁移有很多细节，本来是打算写出来的，后来发现没什么技术含量，都是流水账，直接开源到github好了需要的朋友请直接点击 D2W - Github","categories":[{"name":"PHP","slug":"PHP","permalink":"https://sbcoder.cn/categories/PHP/"}],"tags":[{"name":"Discuz","slug":"Discuz","permalink":"https://sbcoder.cn/tags/Discuz/"},{"name":"Wordpress","slug":"Wordpress","permalink":"https://sbcoder.cn/tags/Wordpress/"}]},{"title":"192TT(192tb)套图吧整站爬虫","slug":"192TT-192tb-套图吧整站爬虫","date":"2019-03-28T08:15:16.000Z","updated":"2019-03-29T06:33:44.000Z","comments":true,"path":"2019/03/28/192tt_Spider.html","link":"","permalink":"https://sbcoder.cn/2019/03/28/192tt_Spider.html","excerpt":"","text":"观察目录结构目标网站：192tb.com网站结构复杂，但也不是太复杂，总体来说都写在导航栏上面了，本以为是new分类下就是所有的文章了，后来发现不是，需要遍历整个导航分类，由于每个分类都有很庞大的资源，因此我决定写成配置文件的形式，建立config.py 1234567891011121314# -*- coding: utf-8 -*-mt = 'https://www.192tb.com/listinfo-1-1.html' # 美图mt1 = 'https://www.192tb.com/meitu/xingganmeinv/' # 性感美女mt2 = 'https://www.192tb.com/meitu/siwameitui/' # 丝袜美腿mt3 = 'https://www.192tb.com/meitu/weimeixiezhen/' # 唯美写真mt4 = 'https://www.192tb.com/meitu/wangluomeinv/' # 网络美女mt5 = 'https://www.192tb.com/meitu/gaoqingmeinv/' # 高清美女mt6 = 'https://www.192tb.com/meitu/motemeinv/' # 模特美女mt7 = 'https://www.192tb.com/meitu/tiyumeinv/' # 体育美女mt8 = 'https://www.192tb.com/meitu/dongmanmeinv/' # 动漫美女mt9 = 'https://www.192tb.com/new/ugirlapp/' # 爱尤物APP/尤果网gc = 'https://www.192tb.com/gc/' # 国产gc1 = 'https://www.192tb.com/gc/bl/' # beautyleg1 顶级分类和二级分类不便多说，这里只是测试并没有收录所有的分类，有兴趣可以自己添加 进入分类页后既是套图封面，从这里可以爬取套图的链接，分类页的底部也是有下一页的选项，可以根据下一页来获取下一个分类页的链接，以此递归，并获取链接 获取到套图链接后发现每个单页面都是需要点击下一张图片来做的，单页面中的图片，使用BeautifulSoup即可轻松获取，由于不知道一套图里面有多少张，我这边使用递归的方式，走到最后一张，即退出递归。 核心代码获取单个套图并下载1234567891011121314151617def getSingleData(url,singleTitle,i = 1): response = requests.get(url) soup = BeautifulSoup(response.text,\"html.parser\") imgUrl = soup.find(id = 'p').find('center').find('img').get('lazysrc') print imgUrl try: j = i + 1 result = '_%s.html'%i in url if result: nextImg = response.url.replace('_%s.html'%i, '_%s.html'%j) else: nextImg = response.url.replace('.html', '_%s.html'%j) # print nextImg downImg(imgUrl,singleTitle,i) getSingleData(nextImg,j) except Exception,e: return 0 获取下一页页面信息123456789101112131415161718192021222324def getPage(url,new = 1,i = 1): print '开始采集第%s页'%i print url response = requests.get(url) soup = BeautifulSoup(response.text,\"html.parser\") for dataUrl in soup.find('div',&#123;'class':'piclist'&#125;).find('ul').find_all('li'): singleDataUrl = 'https://www.192tb.com/'+dataUrl.find('a').get('href') print singleDataUrl try: singleTitle = dataUrl.find('a').find('img').get('alt') except Exception,e: continue print singleTitle getSingleData(singleDataUrl,singleTitle) result = '_%s.html' % i in url j = i + 1 if new != 1: nextPageUrl = url.replace('listinfo-1-%s.html' % i, 'listinfo-1-%s.html' % j) else: if result: nextPageUrl = url.replace('index_%s.html' % i, 'index_%s.html' % j) else: nextPageUrl = url.replace(url, url+'/index_%s.html' % j) getPage(nextPageUrl,new,j) 演示及下载下载地址：Github 192tt演示图","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"福利","slug":"福利","permalink":"https://sbcoder.cn/tags/福利/"}]},{"title":"正则表达式应用，常用取值表（记录）","slug":"正则表达式应用，常用取值表（记录）","date":"2019-03-26T08:11:06.000Z","updated":"2019-03-27T03:50:12.000Z","comments":true,"path":"2019/03/26/Regex_match_note.html","link":"","permalink":"https://sbcoder.cn/2019/03/26/Regex_match_note.html","excerpt":"","text":"正则表达式 查询表 字符 描述 场景 \\ 转义 转义场景 \\ ^ 匹配输入字符串的开始位置。如果设置了 RegExp 对象的 Multiline 属性，^ 也匹配 ‘\\n’ 或 ‘\\r’ 之后的位置。 取a开头的字符串 ^a.* $ 匹配输入字符串的结束位置。如果设置了RegExp 对象的 Multiline 属性，$ 也匹配 ‘\\n’ 或 ‘\\r’ 之前的位置。 取a开头b结尾 ^a.*b$ * 匹配前面的子表达式零次或多次。 zo 能匹配 “z” 以及 “zoo”。 等价于{0,} + 匹配前面的子表达式一次或多次。 ‘zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,} ? 匹配前面的子表达式零次或一次. “do(es)?” 可以匹配 “do” 或 “does” 中的”do” 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。 ‘o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。 {n,} n 是一个非负整数。至少匹配n 次。 ‘o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。’o{1,}’ 等价于 ‘o+’。’o{0,}’ 则等价于 ‘o*’ {n,m} m 和 n 均为非负整数，其中n &lt;= m。最少匹配 n 次且最多匹配 m 次。 “o{1,3}” 将匹配 “fooooood” 中的前三个 o。’o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。 ? 当 该字符紧跟在任何一个其他限制符 (*, +, ?, {n}, {n,}, {n,m}) 后面时，匹配模式是非贪婪的。 非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。对于字符串 “oooo”，’o+?’ 将匹配单个 “o”，而 ‘o+’ 将匹配所有 ‘o’。 . 匹配除 “\\n” 之外的任何单个字符。 要匹配包括 ‘\\n’ 在内的任何字符，请使用象 ‘[.\\n]’ 的模式。 x&#124;y 匹配 x 或 y。 ‘z&#124;food’ 能匹配 “z” 或 “food”。’(z&#124;f)ood’ 则匹配 “zood” 或 “food”。 [xyz] 字符集合。匹配所包含的任意一个字符。 ‘[abc]’可以匹配 “plain” 中的 ‘a’。 [^xyz] 取反，匹配未包含的任意字符。 ‘?[^abc]’ 可以匹配 “plain” 中的’p’。 [a-z] 字符范围。匹配指定范围内的任意字符。 ‘[a-z]’ 可以匹配 ‘a’ 到 ‘z’ 范围内的任意小写字母字符。 \\b 匹配一个单词边界，也就是指单词和空格间的位置。 ‘er\\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。 \\B 匹配非单词边界 ‘er\\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。 \\cx 匹配由 x 指明的控制字符。 \\cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。 \\d 匹配一个数字字符 等价于 [0-9]。 \\D 匹配一个非数字字符。 等价于 [^0-9]。 \\f 匹配一个换页符。 等价于 \\x0c 和 \\cL。 \\n 匹配一个换行符。 等价于 \\x0a 和 \\cJ。 \\r 匹配一个回车符。 等价于 \\x0d 和 \\cM。 \\s 匹配任何空白字符，包括空格、制表符、换页符等等。 等价于 [ \\f\\n\\r\\t\\v]。 \\S 匹配任何非空白字符。 等价于 [^ \\f\\n\\r\\t\\v]。 \\t 匹配一个制表符。 等价于 \\x09 和 \\cI。 \\v 匹配一个垂直制表符。 等价于 \\x0b 和 \\cK。 \\w 匹配包括下划线的任何单词字符。 等价于’[A-Za-z0-9_]’。 \\W 匹配任何非单词字符。 等价于 ‘[^A-Za-z0-9_]’。 \\xn 匹配十六进制数 ‘\\x41’ 匹配 “A”。’\\x041’ 则等价于 ‘\\x04’ &amp; “1”。正则表达式中可以使用 ASCII 编码。. \\num 匹配 一个正整数。对所获取的匹配的引用。 ‘(.)\\1’ 匹配两个连续的相同字符。 \\n 标识一个八进制转义值或一个向后引用。 如果 \\n 之前至少 n 个获取的子表达式，则 n 为向后引用。否则，如果 n 为八进制数字 (0-7)，则 n 为一个八进制转义值。 \\nm 标 识一个八进制转义值或一个向后引用。 如果 \\nm 之前至少有 nm 个获得子表达式，则 nm 为向后引用。如果 \\nm 之前至少有 n 个获取，则 n 为一个后跟文字 m 的向后引用。如果前面的条件都不满足，若 n 和 m 均为八进制数字 (0-7)，则 \\nm 将匹配八进制转义值 nm。 \\nml 匹配八进制数 如果 n 为八进制数字 (0-3)，且 m 和 l 均为八进制数字 (0-7)，则匹配八进制转义值 nml。 \\un 匹配 n，其中 n 是一个用四个十六进制数字表示的 Unicode 字符。 \\u00A9 匹配版权符号 。 常用案例演示123# -*- coding:utf-8 -*-import restr1 = 'ai0by123' 提取a开头的字符串1regexStr = \"^a.*\" 提取a开头b结尾字符串1regexStr = \"^a.*3$\" 提取最右边符合条件的值,贪婪1regexStr = \".*(a.*b).*\" # 贪婪，取a到b之间，右边开始取，取最右边符合条件的 提取最左边符合条件的值，非贪婪1regexStr = \".*?(a.*?b).*\" # 非贪婪，取a到b之间的值含a和b，从左往右只取一次 提取符合集合内的值，或运算1regexStr = \"((ai00000by|ai0by)123)\" # 或运算，符合其中一种即可 提取出生日期123456str1 = 'XXX 出生于2008年12月6日'str1 = 'XXX 出生于2008/12/6'str1 = 'XXX 出生于2008-12-6'str1 = 'XXX 出生于2008-12-06'str1 = 'XXX 出生于2008-12'regexStr = \".*出生于(\\d&#123;4&#125;[年/-]\\d&#123;1,2&#125;([月/-]\\d&#123;1,2&#125;|[月/-]$|$))\" 提取图片url,其他网站同理1234567str1 = '地址：https://www.ttbcdn.com/d/file/p/2018-02-17/g4edlvxmmyi9627.jpg'# 取整串地址regexStr = \".*https.*jpg$\"# 取XXX.jpg png gif 等regexStr = \".*/(.*.(jpg|gif|png))$\"# 取2018-02-17/g4edlvxmmyi9627.jpg png gif等regexStr = \".*/(\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;/(.*.(jpg|gif|png)))$\" 收尾提取字符串12345reMatch = re.match(regexStr,str1)if reMatch: print (reMatch.group(1))else: print('No')","categories":[{"name":"note","slug":"note","permalink":"https://sbcoder.cn/categories/note/"}],"tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"https://sbcoder.cn/tags/正则表达式/"},{"name":"笔记","slug":"笔记","permalink":"https://sbcoder.cn/tags/笔记/"}]},{"title":"博客迁移说明","slug":"博客迁移说明","date":"2019-03-25T05:12:58.000Z","updated":"2019-04-12T14:57:49.000Z","comments":true,"path":"2019/03/25/Hello_world.html","link":"","permalink":"https://sbcoder.cn/2019/03/25/Hello_world.html","excerpt":"","text":"关于迁移博客总是在起起伏伏，关了又开，开了又关中反复，这一次，我将风向标博客放在了Github Page上。程序采用了当下比较流行的静态博客程序 Hexo ，Hexo其实是一个非常好的程序，但由于我经常换电脑，以前用hexo搭建的博客数据丢失了很多次，后经过更换为Wordpress，Typecho之类的开源博客程序后，我又回到了 Hexo 的怀抱，可能是真的懒得折腾了，上了年纪？这次我将源代码都备份好了，应该会长期更新，有什么好的东西我应该会分享出来，主打原创~可能之前认识我的人也很少，但我这个域名还是很好记的，sb coder 也是一种自嘲吧，有想跟我交流技术或者有外包工作介绍给我的，我的微信与域名同号~多的不说了，我将尽我所能，一周至少写一篇文章，可能有时候晚上回家写一点，一天写一点，一周下来也能写不少，希望各位监督~ 关于我我是谁，职业是PHP，爱好Python，云服务器爱好者支付接口对接，可以定制各类免签约支付接口，微信支付宝，有想法的朋友可以联系我 Telegram : ai0by 承接业务支付相关业务，爬虫(几乎不要钱，练手)，PHP程序开发服务器环境配置，PHP程序修改，PHP BUG排查","categories":[],"tags":[{"name":"ai0by","slug":"ai0by","permalink":"https://sbcoder.cn/tags/ai0by/"}]},{"title":"岛国推特妹子图爬虫","slug":"岛国推特妹子图爬虫","date":"2019-03-22T03:54:58.000Z","updated":"2019-03-22T05:11:10.000Z","comments":true,"path":"2019/03/22/Japan_Twitter_Spider.html","link":"","permalink":"https://sbcoder.cn/2019/03/22/Japan_Twitter_Spider.html","excerpt":"","text":"LOC的大佬们分享了一个网站，收集了很多岛国的妹子图和她们的推特地址：岛国妹子推特推特不是很感兴趣，就爬一下图片好了~ 爬虫介绍爬虫环境： Python2.7.9 可更替为3，自行更替 BeautifulSoup4 requests 代码：12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-from bs4 import BeautifulSoupimport requestsimport urllib2import randomdef spy(url): req = urllib2.Request(url) req = urllib2.urlopen(req) page = req.read() soup = BeautifulSoup(page, \"html.parser\") for imgSoup in soup.find_all('div', &#123;\"class\": \"row\"&#125;): for i in imgSoup.find_all('div', &#123;'class': 'photo'&#125;): for j in i.find('div', &#123;'class': 'photo-link-outer'&#125;).find('a').find_all('img'): img = j.get(\"src\") print img str = random.sample('zyxwvutsrqponmlkjihgfedcba', 6) downImg(img, str) nexturl = soup.find('p',&#123;'class':'go-to-next-page'&#125;) nexturl = nexturl.find('a').get('href') pageurl = \"http://jigadori.fkoji.com\"+nexturl spy(pageurl)def downImg(img,m): try: r = requests.get(img) except Exception , e: print \"图片获取失败\" return with open('./img/good%s.jpg' % m, 'wb') as f: f.write(r.content)if __name__ == '__main__': url = \"http://jigadori.fkoji.com\" spy(url) 整体思路看一下，网页构造，发现首页底部有下一页标签，BeautifulSoup取Class取值递归获取下一页地址图片同上整体难度不高，有兴趣的可以拿这个网站练练手~ 演示截图 演示数据1 演示数据2","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"福利","slug":"福利","permalink":"https://sbcoder.cn/tags/福利/"}]},{"title":"Python+selenium针对网银控件过登录取数据","slug":"Python-selenium针对网银控件过登录取数据","date":"2019-03-21T08:53:30.000Z","updated":"2019-03-22T03:10:52.000Z","comments":true,"path":"2019/03/21/Python_WY_Spider.html","link":"","permalink":"https://sbcoder.cn/2019/03/21/Python_WY_Spider.html","excerpt":"","text":"Python获取WY数据的方法总结： 由于WY控件加密方式相比较一般网站比较特殊，需在驱动层进行操作 由于WY控件的特殊性，获取交易数据的时候需要传递KEY (以CCB为例) 由于取数据时的问题，所有操作均应该在驱动层完成 要想实现实时更新数据，需要不断地登录，大部分WY都有强制退出操作 理清思路登录的过程中，由于安全控件的限制，需要绕过登录限制，此处思路借鉴了 爬虫应对银行安全控件，由此可知，需要绕过登陆限制需从驱动层入手 两种方案 Python可以使用 Win32api 模块来模拟键盘指令，类似于按键精灵的概念 Python使用 Photomjs 无界面浏览器配合Selenium Webdirver 尝试使用Win32api时，由于需要配合鼠标操作，需要获取句柄坐标，且开发难度较高，尝试更换另一种方式更换 Photomjs ，模拟登录时发现 Photomjs 并没有附带安全控件，所传输的值不会自动加密，尝试更换为 ChromDriver使用 ChromDriver 尝试模拟登陆 123456username = browser.find_element_by_name('USERID')username.send_keys(username)password = browser.find_element_by_name('LOGPASS')password.send_keys(password)tjButton = browser.find_element_by_id('loginButton')tjButton.click() 登录成功！ 当越过了登录后就需要获取交易信息，交易信息这一块，CCB的查询地址附带了一个SKEY，每次查询信息的时候都需要一个SKEY验证，如果不正确将不会返回正确的结果！如何获取SKEY，涉及到WY的信息，这里不便细说（PS：细心地同学一定可以找到） 取到SKEY后即可构造查询地址，然后使用WebDriver模拟访问需要注意的一点是，WY的大部分数据均是用iframe嵌套的，因此需要多处过iframe 演示代码1234# 定位到iframeiframe = browser.find_element_by_id(\"iframe\")# 切换到iframebrowser.switch_to_frame(iframe) 取数据这一块不多说，每个WY也都不同获取到数据后就是数据处理了，根据系统不同，我这里直接使用Python向固定地址POST传值 取数据后为了获取实时数据，需要定时向固定地址提交数据，大部分的WY都有长时间自动登出的骚操作，对此，思路也很多 大致几个想法 自动刷新，保持登录状态 重复登录，更换SKEY，反复操作 模拟点击，保持登录状态 自动刷新方案在一开始就失败了，多次频繁的刷新，会导致弹出手机验证码重复登录，由于上次的失败，设置了延时，效果还可以，只是数据总会有延迟模拟点击，无效，仍然会自动登出 采用重复登录的方式，递归实现！ 综上所述，我们可以将交易流程如此划分： WY数据.jpg 成果演示 WY演示.jpg Tips:本文仅做思路分享，切勿用在实际生产环境！","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"网银","slug":"网银","permalink":"https://sbcoder.cn/tags/网银/"},{"name":"selenium","slug":"selenium","permalink":"https://sbcoder.cn/tags/selenium/"}]}]}