{"meta":{"title":"风向标 | 分享与创造","subtitle":null,"description":null,"author":"ai0by","url":"https://sbcoder.cn","root":"/"},"pages":[{"title":"api","date":"2019-04-03T02:17:31.000Z","updated":"2019-04-04T07:33:08.000Z","comments":true,"path":"api/index.html","permalink":"https://sbcoder.cn/api/index.html","excerpt":"","text":"风向标API合集 名称 作用 接口 微博图床api 远程图片上传到微博图床 https://sbcoder.cn/api/sinaImg.html 生成二维码 将地址转换为二维码图片 https://sbcoder.cn/api/qrcode.html 更多api请持续关注…"},{"title":"api","date":"2019-04-04T07:27:21.000Z","updated":"2019-04-04T07:31:54.000Z","comments":true,"path":"api/qrcode.html","permalink":"https://sbcoder.cn/api/qrcode.html","excerpt":"","text":"二维码生成介绍使用场景很多，简单来讲，给我地址，我给你图，直接将地址放在img标签内即可~ 参数说明以GET的方式提交到：https://api.0161.org/qrcode/qrcode.php 参数 数值类型 示例 是否必传 url String https://sbcoder.cn 是 err String L (L,M,Q,H四种对应容错级别，不传默认L) 否 size String 7 (可以选择1~9999之间的值，对应不同大小，默认7) 否 logo String https://sbcoder.cn/img/avatar.jpg 否 返回值类型 : 直接返回图片 演示直接访问以下地址 12https://api.0161.org/qrcode/qrcode.php?url=https://sbcoder.cnhttps://api.0161.org/qrcode/qrcode.php?url=https://sbcoder.cn&amp;err=L&amp;size=7&amp;logo=https://sbcoder.cn/img/avatar.jpg 示例图片"},{"title":"api","date":"2019-04-04T07:27:21.000Z","updated":"2019-04-04T07:27:58.000Z","comments":true,"path":"api/sinaImg.html","permalink":"https://sbcoder.cn/api/sinaImg.html","excerpt":"","text":"微博图床-远程图片上传api介绍我们在使用爬虫相关内容的时候，存放图片时往往会遇到图片尺寸过大，存储不方便等问题，这时候，存放在一个永久存储的云上面就很有必要，微博是一个不限流量，全球CDN的图床~微博也是有缺点的，他并不是一个易于管理的图床，仅限于存放图片但不能管理图片，如果希望使用可以管理的图床，可以参考使用自建图床，参考我的:FuliCOSIMG 参数说明以GET的方式提交到：https://api.0161.org/sinaimg/sinaImg.php 传递参数类型：GET POST 参数 数值类型 示例 是否必传 url String https://sbcoder.cn/img/avatar.jpg 是 返回参数类型 ： JSON 演示示例:1&#123;\"large\":\"http://ww2.sinaimg.cn/bmiddle/0062WdSely1g1p8mlciyrg30390120jl.gif\"&#125; PHP DEMO12345678910111213141516171819$data = array( 'url' =&gt; \"https://sbcoder.cn/img/avatar.jpg\", );echo curlPost(\"https://api.0161.org/sinaimg/sinaImg.php\",$data);function curlPost($url,$res)&#123; $curl = curl_init(); curl_setopt($curl, CURLOPT_URL, $url); curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, 0); curl_setopt($curl, CURLOPT_FOLLOWLOCATION, 1); curl_setopt($curl, CURLOPT_AUTOREFERER, 1); curl_setopt($curl, CURLOPT_POST, 1); curl_setopt($curl, CURLOPT_POSTFIELDS, http_build_query($res)); curl_setopt($curl, CURLOPT_TIMEOUT, 30); curl_setopt($curl, CURLOPT_HEADER, 0); curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1); $result = curl_exec($curl); curl_close($curl); return $result;&#125;"},{"title":"categories","date":"2019-03-21T07:11:04.000Z","updated":"2019-03-21T07:11:32.000Z","comments":false,"path":"categories/index.html","permalink":"https://sbcoder.cn/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-03-21T05:14:24.000Z","updated":"2019-03-21T05:14:52.000Z","comments":false,"path":"tags/index.html","permalink":"https://sbcoder.cn/tags/index.html","excerpt":"","text":""},{"title":"Links","date":"2019-04-22T12:14:24.000Z","updated":"2019-05-07T11:06:45.000Z","comments":true,"path":"custom/index.html","permalink":"https://sbcoder.cn/custom/index.html","excerpt":"","text":"我的朋友 - 排名不分先后 - 二逼哥博客 - OwCrypto加密货币百科 - Huas Leung’s Blog - whan的博客 申请友情链接 直接在下面留言即可！ 要求： - 正规站点 - 博客类优先 - 技术类站点优先 本站链接格式 - 风向标博客 - https://sbcoder.cn"}],"posts":[{"title":"Discuz会员数据与Wordpress互通","slug":"Discuz会员数据与Wordpress互通","date":"2019-04-11T12:06:06.000Z","updated":"2019-05-13T14:57:58.000Z","comments":true,"path":"2019/04/11/Discuz-Userinfo-To-Wordpress.html","link":"","permalink":"https://sbcoder.cn/2019/04/11/Discuz-Userinfo-To-Wordpress.html","excerpt":"","text":"情景这个情景可能遇到的也不在少数，不想舍弃用户数据，还想让用户无需注册在新站保留账号。实际当我们在迁移的时候，稍微了解数据库的同学应该明白想要迁移用户数据只需要迁移用户数据表即可。实际上我也是这么做的，但是中途遇到了几个小问题，这里我总结一下！ Discuz用户密码加密算法Discuz的用户信息都存放在 ‘pre_common_member‘ 表里，包含了我们需要转移的 邮箱,用户名,密码,积分,ip 等各类信息那么很简单了，便利这个表再插入到Wordpress表内即可但在导入表之前需要先测试一下用户数据是否匹配以示严谨~当我测试密码匹配的时候发现，这里的密码似乎并不匹配，首先我想到的就是应该是加盐了，但是纵观整个 ‘pre_common_member‘ 表，似乎并没有该有的字段网上找了一圈发现Discuz的用户真实密码是存在 ‘pre_ucenter_members‘ 表内的，’pre_common_member‘ 表内的密码我现在还不知道有什么用处，但至少跟我们需要迁移的数据没什么关联。从 ‘pre_ucenter_members‘ 表中找到了我们需要的 ‘salt‘ 字段，经过测试得出Discuz的密码加密算法为1md5(md5('password').'salt'); tips: Discuz里面的salt是一个6位的 数字+字母 随机数 Wordpress用户密码加密算法搞定了DZ的加密算法后，那么如何将DZ的用户信息插入到WP里面就很重要了，打开 Wordpress 的数据库找到 ‘wp-user‘表，找到 ‘user_pass‘ 字段，发现里面加密的内容似乎无迹可寻。实际上，Wordpress的加密是使用了 phpass 类来加密的，由 phpass 加密的密码具有不可逆性，所以想要破解是不可能了，这里简单说一下 phpass 的加密算法目前我们的PHP版本应该都在5以上，所以前缀是一样的 $P$B 大致写出来如下：12345$count = rand(1,8);$hash = md5($salt . $password, TRUE);while($count--)&#123; $hash = md5($hash . $password, TRUE);&#125; 看起来是不是很强，我们无法破解这样的密码，实际使用中，我们也可以使用 phpass 来做密码加密，让我们的数据库更加的安全~然而我们数据迁移时其实完全可以避免这种问题，Wordpress是保留了md5加密的形式的，如果 ‘user_pass‘ 字段里面存储的是md5加密的32值，wordpress也可以登录成功，并且再登陆后会将 ‘user_pass‘ 字段修改为 phpass 加密的格式，是不是很人性化呢。综上所述，我们无需解出来wordpress的加密算法，我们也解不出来~ 开始迁移用户数据关于迁移有很多细节，本来是打算写出来的，后来发现没什么技术含量，都是流水账，直接开源到github好了需要的朋友请直接点击 D2W - Github","categories":[{"name":"PHP","slug":"PHP","permalink":"https://sbcoder.cn/categories/PHP/"}],"tags":[{"name":"Discuz","slug":"Discuz","permalink":"https://sbcoder.cn/tags/Discuz/"},{"name":"Wordpress","slug":"Wordpress","permalink":"https://sbcoder.cn/tags/Wordpress/"}]},{"title":"192TT(192tb)套图吧整站爬虫","slug":"192TT-192tb-套图吧整站爬虫","date":"2019-03-28T08:15:16.000Z","updated":"2019-03-29T06:33:44.000Z","comments":true,"path":"2019/03/28/192tt_Spider.html","link":"","permalink":"https://sbcoder.cn/2019/03/28/192tt_Spider.html","excerpt":"","text":"观察目录结构目标网站：192tb.com网站结构复杂，但也不是太复杂，总体来说都写在导航栏上面了，本以为是new分类下就是所有的文章了，后来发现不是，需要遍历整个导航分类，由于每个分类都有很庞大的资源，因此我决定写成配置文件的形式，建立config.py 1234567891011121314# -*- coding: utf-8 -*-mt = 'https://www.192tb.com/listinfo-1-1.html' # 美图mt1 = 'https://www.192tb.com/meitu/xingganmeinv/' # 性感美女mt2 = 'https://www.192tb.com/meitu/siwameitui/' # 丝袜美腿mt3 = 'https://www.192tb.com/meitu/weimeixiezhen/' # 唯美写真mt4 = 'https://www.192tb.com/meitu/wangluomeinv/' # 网络美女mt5 = 'https://www.192tb.com/meitu/gaoqingmeinv/' # 高清美女mt6 = 'https://www.192tb.com/meitu/motemeinv/' # 模特美女mt7 = 'https://www.192tb.com/meitu/tiyumeinv/' # 体育美女mt8 = 'https://www.192tb.com/meitu/dongmanmeinv/' # 动漫美女mt9 = 'https://www.192tb.com/new/ugirlapp/' # 爱尤物APP/尤果网gc = 'https://www.192tb.com/gc/' # 国产gc1 = 'https://www.192tb.com/gc/bl/' # beautyleg1 顶级分类和二级分类不便多说，这里只是测试并没有收录所有的分类，有兴趣可以自己添加 进入分类页后既是套图封面，从这里可以爬取套图的链接，分类页的底部也是有下一页的选项，可以根据下一页来获取下一个分类页的链接，以此递归，并获取链接 获取到套图链接后发现每个单页面都是需要点击下一张图片来做的，单页面中的图片，使用BeautifulSoup即可轻松获取，由于不知道一套图里面有多少张，我这边使用递归的方式，走到最后一张，即退出递归。 核心代码获取单个套图并下载1234567891011121314151617def getSingleData(url,singleTitle,i = 1): response = requests.get(url) soup = BeautifulSoup(response.text,\"html.parser\") imgUrl = soup.find(id = 'p').find('center').find('img').get('lazysrc') print imgUrl try: j = i + 1 result = '_%s.html'%i in url if result: nextImg = response.url.replace('_%s.html'%i, '_%s.html'%j) else: nextImg = response.url.replace('.html', '_%s.html'%j) # print nextImg downImg(imgUrl,singleTitle,i) getSingleData(nextImg,j) except Exception,e: return 0 获取下一页页面信息123456789101112131415161718192021222324def getPage(url,new = 1,i = 1): print '开始采集第%s页'%i print url response = requests.get(url) soup = BeautifulSoup(response.text,\"html.parser\") for dataUrl in soup.find('div',&#123;'class':'piclist'&#125;).find('ul').find_all('li'): singleDataUrl = 'https://www.192tb.com/'+dataUrl.find('a').get('href') print singleDataUrl try: singleTitle = dataUrl.find('a').find('img').get('alt') except Exception,e: continue print singleTitle getSingleData(singleDataUrl,singleTitle) result = '_%s.html' % i in url j = i + 1 if new != 1: nextPageUrl = url.replace('listinfo-1-%s.html' % i, 'listinfo-1-%s.html' % j) else: if result: nextPageUrl = url.replace('index_%s.html' % i, 'index_%s.html' % j) else: nextPageUrl = url.replace(url, url+'/index_%s.html' % j) getPage(nextPageUrl,new,j) 演示及下载下载地址：Github 192tt演示图","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"福利","slug":"福利","permalink":"https://sbcoder.cn/tags/福利/"}]},{"title":"正则表达式应用，常用取值表（记录）","slug":"正则表达式应用，常用取值表（记录）","date":"2019-03-26T08:11:06.000Z","updated":"2019-03-27T03:50:12.000Z","comments":true,"path":"2019/03/26/Regex_match_note.html","link":"","permalink":"https://sbcoder.cn/2019/03/26/Regex_match_note.html","excerpt":"","text":"正则表达式 查询表 字符 描述 场景 \\ 转义 转义场景 \\ ^ 匹配输入字符串的开始位置。如果设置了 RegExp 对象的 Multiline 属性，^ 也匹配 ‘\\n’ 或 ‘\\r’ 之后的位置。 取a开头的字符串 ^a.* $ 匹配输入字符串的结束位置。如果设置了RegExp 对象的 Multiline 属性，$ 也匹配 ‘\\n’ 或 ‘\\r’ 之前的位置。 取a开头b结尾 ^a.*b$ * 匹配前面的子表达式零次或多次。 zo 能匹配 “z” 以及 “zoo”。 等价于{0,} + 匹配前面的子表达式一次或多次。 ‘zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,} ? 匹配前面的子表达式零次或一次. “do(es)?” 可以匹配 “do” 或 “does” 中的”do” 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。 ‘o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。 {n,} n 是一个非负整数。至少匹配n 次。 ‘o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。’o{1,}’ 等价于 ‘o+’。’o{0,}’ 则等价于 ‘o*’ {n,m} m 和 n 均为非负整数，其中n &lt;= m。最少匹配 n 次且最多匹配 m 次。 “o{1,3}” 将匹配 “fooooood” 中的前三个 o。’o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。 ? 当 该字符紧跟在任何一个其他限制符 (*, +, ?, {n}, {n,}, {n,m}) 后面时，匹配模式是非贪婪的。 非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。对于字符串 “oooo”，’o+?’ 将匹配单个 “o”，而 ‘o+’ 将匹配所有 ‘o’。 . 匹配除 “\\n” 之外的任何单个字符。 要匹配包括 ‘\\n’ 在内的任何字符，请使用象 ‘[.\\n]’ 的模式。 x&#124;y 匹配 x 或 y。 ‘z&#124;food’ 能匹配 “z” 或 “food”。’(z&#124;f)ood’ 则匹配 “zood” 或 “food”。 [xyz] 字符集合。匹配所包含的任意一个字符。 ‘[abc]’可以匹配 “plain” 中的 ‘a’。 [^xyz] 取反，匹配未包含的任意字符。 ‘?[^abc]’ 可以匹配 “plain” 中的’p’。 [a-z] 字符范围。匹配指定范围内的任意字符。 ‘[a-z]’ 可以匹配 ‘a’ 到 ‘z’ 范围内的任意小写字母字符。 \\b 匹配一个单词边界，也就是指单词和空格间的位置。 ‘er\\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。 \\B 匹配非单词边界 ‘er\\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。 \\cx 匹配由 x 指明的控制字符。 \\cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。 \\d 匹配一个数字字符 等价于 [0-9]。 \\D 匹配一个非数字字符。 等价于 [^0-9]。 \\f 匹配一个换页符。 等价于 \\x0c 和 \\cL。 \\n 匹配一个换行符。 等价于 \\x0a 和 \\cJ。 \\r 匹配一个回车符。 等价于 \\x0d 和 \\cM。 \\s 匹配任何空白字符，包括空格、制表符、换页符等等。 等价于 [ \\f\\n\\r\\t\\v]。 \\S 匹配任何非空白字符。 等价于 [^ \\f\\n\\r\\t\\v]。 \\t 匹配一个制表符。 等价于 \\x09 和 \\cI。 \\v 匹配一个垂直制表符。 等价于 \\x0b 和 \\cK。 \\w 匹配包括下划线的任何单词字符。 等价于’[A-Za-z0-9_]’。 \\W 匹配任何非单词字符。 等价于 ‘[^A-Za-z0-9_]’。 \\xn 匹配十六进制数 ‘\\x41’ 匹配 “A”。’\\x041’ 则等价于 ‘\\x04’ &amp; “1”。正则表达式中可以使用 ASCII 编码。. \\num 匹配 一个正整数。对所获取的匹配的引用。 ‘(.)\\1’ 匹配两个连续的相同字符。 \\n 标识一个八进制转义值或一个向后引用。 如果 \\n 之前至少 n 个获取的子表达式，则 n 为向后引用。否则，如果 n 为八进制数字 (0-7)，则 n 为一个八进制转义值。 \\nm 标 识一个八进制转义值或一个向后引用。 如果 \\nm 之前至少有 nm 个获得子表达式，则 nm 为向后引用。如果 \\nm 之前至少有 n 个获取，则 n 为一个后跟文字 m 的向后引用。如果前面的条件都不满足，若 n 和 m 均为八进制数字 (0-7)，则 \\nm 将匹配八进制转义值 nm。 \\nml 匹配八进制数 如果 n 为八进制数字 (0-3)，且 m 和 l 均为八进制数字 (0-7)，则匹配八进制转义值 nml。 \\un 匹配 n，其中 n 是一个用四个十六进制数字表示的 Unicode 字符。 \\u00A9 匹配版权符号 。 常用案例演示123# -*- coding:utf-8 -*-import restr1 = 'ai0by123' 提取a开头的字符串1regexStr = \"^a.*\" 提取a开头b结尾字符串1regexStr = \"^a.*3$\" 提取最右边符合条件的值,贪婪1regexStr = \".*(a.*b).*\" # 贪婪，取a到b之间，右边开始取，取最右边符合条件的 提取最左边符合条件的值，非贪婪1regexStr = \".*?(a.*?b).*\" # 非贪婪，取a到b之间的值含a和b，从左往右只取一次 提取符合集合内的值，或运算1regexStr = \"((ai00000by|ai0by)123)\" # 或运算，符合其中一种即可 提取出生日期123456str1 = 'XXX 出生于2008年12月6日'str1 = 'XXX 出生于2008/12/6'str1 = 'XXX 出生于2008-12-6'str1 = 'XXX 出生于2008-12-06'str1 = 'XXX 出生于2008-12'regexStr = \".*出生于(\\d&#123;4&#125;[年/-]\\d&#123;1,2&#125;([月/-]\\d&#123;1,2&#125;|[月/-]$|$))\" 提取图片url,其他网站同理1234567str1 = '地址：https://www.ttbcdn.com/d/file/p/2018-02-17/g4edlvxmmyi9627.jpg'# 取整串地址regexStr = \".*https.*jpg$\"# 取XXX.jpg png gif 等regexStr = \".*/(.*.(jpg|gif|png))$\"# 取2018-02-17/g4edlvxmmyi9627.jpg png gif等regexStr = \".*/(\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;/(.*.(jpg|gif|png)))$\" 收尾提取字符串12345reMatch = re.match(regexStr,str1)if reMatch: print (reMatch.group(1))else: print('No')","categories":[{"name":"note","slug":"note","permalink":"https://sbcoder.cn/categories/note/"}],"tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"https://sbcoder.cn/tags/正则表达式/"},{"name":"笔记","slug":"笔记","permalink":"https://sbcoder.cn/tags/笔记/"}]},{"title":"博客迁移说明","slug":"博客迁移说明","date":"2019-03-25T05:12:58.000Z","updated":"2019-04-12T14:57:49.000Z","comments":true,"path":"2019/03/25/Hello_world.html","link":"","permalink":"https://sbcoder.cn/2019/03/25/Hello_world.html","excerpt":"","text":"关于迁移博客总是在起起伏伏，关了又开，开了又关中反复，这一次，我将风向标博客放在了Github Page上。程序采用了当下比较流行的静态博客程序 Hexo ，Hexo其实是一个非常好的程序，但由于我经常换电脑，以前用hexo搭建的博客数据丢失了很多次，后经过更换为Wordpress，Typecho之类的开源博客程序后，我又回到了 Hexo 的怀抱，可能是真的懒得折腾了，上了年纪？这次我将源代码都备份好了，应该会长期更新，有什么好的东西我应该会分享出来，主打原创~可能之前认识我的人也很少，但我这个域名还是很好记的，sb coder 也是一种自嘲吧，有想跟我交流技术或者有外包工作介绍给我的，我的微信与域名同号~多的不说了，我将尽我所能，一周至少写一篇文章，可能有时候晚上回家写一点，一天写一点，一周下来也能写不少，希望各位监督~ 关于我我是谁，职业是PHP，爱好Python，云服务器爱好者支付接口对接，可以定制各类免签约支付接口，微信支付宝，有想法的朋友可以联系我 Telegram : ai0by 承接业务支付相关业务，爬虫(几乎不要钱，练手)，PHP程序开发服务器环境配置，PHP程序修改，PHP BUG排查","categories":[],"tags":[{"name":"ai0by","slug":"ai0by","permalink":"https://sbcoder.cn/tags/ai0by/"}]},{"title":"岛国推特妹子图爬虫","slug":"岛国推特妹子图爬虫","date":"2019-03-22T03:54:58.000Z","updated":"2019-03-22T05:11:10.000Z","comments":true,"path":"2019/03/22/Japan_Twitter_Spider.html","link":"","permalink":"https://sbcoder.cn/2019/03/22/Japan_Twitter_Spider.html","excerpt":"","text":"LOC的大佬们分享了一个网站，收集了很多岛国的妹子图和她们的推特地址：岛国妹子推特推特不是很感兴趣，就爬一下图片好了~ 爬虫介绍爬虫环境： Python2.7.9 可更替为3，自行更替 BeautifulSoup4 requests 代码：12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-from bs4 import BeautifulSoupimport requestsimport urllib2import randomdef spy(url): req = urllib2.Request(url) req = urllib2.urlopen(req) page = req.read() soup = BeautifulSoup(page, \"html.parser\") for imgSoup in soup.find_all('div', &#123;\"class\": \"row\"&#125;): for i in imgSoup.find_all('div', &#123;'class': 'photo'&#125;): for j in i.find('div', &#123;'class': 'photo-link-outer'&#125;).find('a').find_all('img'): img = j.get(\"src\") print img str = random.sample('zyxwvutsrqponmlkjihgfedcba', 6) downImg(img, str) nexturl = soup.find('p',&#123;'class':'go-to-next-page'&#125;) nexturl = nexturl.find('a').get('href') pageurl = \"http://jigadori.fkoji.com\"+nexturl spy(pageurl)def downImg(img,m): try: r = requests.get(img) except Exception , e: print \"图片获取失败\" return with open('./img/good%s.jpg' % m, 'wb') as f: f.write(r.content)if __name__ == '__main__': url = \"http://jigadori.fkoji.com\" spy(url) 整体思路看一下，网页构造，发现首页底部有下一页标签，BeautifulSoup取Class取值递归获取下一页地址图片同上整体难度不高，有兴趣的可以拿这个网站练练手~ 演示截图 演示数据1 演示数据2","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"福利","slug":"福利","permalink":"https://sbcoder.cn/tags/福利/"}]},{"title":"Python+selenium针对网银控件过登录取数据","slug":"Python-selenium针对网银控件过登录取数据","date":"2019-03-21T08:53:30.000Z","updated":"2019-03-22T03:10:52.000Z","comments":true,"path":"2019/03/21/Python_WY_Spider.html","link":"","permalink":"https://sbcoder.cn/2019/03/21/Python_WY_Spider.html","excerpt":"","text":"Python获取WY数据的方法总结： 由于WY控件加密方式相比较一般网站比较特殊，需在驱动层进行操作 由于WY控件的特殊性，获取交易数据的时候需要传递KEY (以CCB为例) 由于取数据时的问题，所有操作均应该在驱动层完成 要想实现实时更新数据，需要不断地登录，大部分WY都有强制退出操作 理清思路登录的过程中，由于安全控件的限制，需要绕过登录限制，此处思路借鉴了 爬虫应对银行安全控件，由此可知，需要绕过登陆限制需从驱动层入手 两种方案 Python可以使用 Win32api 模块来模拟键盘指令，类似于按键精灵的概念 Python使用 Photomjs 无界面浏览器配合Selenium Webdirver 尝试使用Win32api时，由于需要配合鼠标操作，需要获取句柄坐标，且开发难度较高，尝试更换另一种方式更换 Photomjs ，模拟登录时发现 Photomjs 并没有附带安全控件，所传输的值不会自动加密，尝试更换为 ChromDriver使用 ChromDriver 尝试模拟登陆 123456username = browser.find_element_by_name('USERID')username.send_keys(username)password = browser.find_element_by_name('LOGPASS')password.send_keys(password)tjButton = browser.find_element_by_id('loginButton')tjButton.click() 登录成功！ 当越过了登录后就需要获取交易信息，交易信息这一块，CCB的查询地址附带了一个SKEY，每次查询信息的时候都需要一个SKEY验证，如果不正确将不会返回正确的结果！如何获取SKEY，涉及到WY的信息，这里不便细说（PS：细心地同学一定可以找到） 取到SKEY后即可构造查询地址，然后使用WebDriver模拟访问需要注意的一点是，WY的大部分数据均是用iframe嵌套的，因此需要多处过iframe 演示代码1234# 定位到iframeiframe = browser.find_element_by_id(\"iframe\")# 切换到iframebrowser.switch_to_frame(iframe) 取数据这一块不多说，每个WY也都不同获取到数据后就是数据处理了，根据系统不同，我这里直接使用Python向固定地址POST传值 取数据后为了获取实时数据，需要定时向固定地址提交数据，大部分的WY都有长时间自动登出的骚操作，对此，思路也很多 大致几个想法 自动刷新，保持登录状态 重复登录，更换SKEY，反复操作 模拟点击，保持登录状态 自动刷新方案在一开始就失败了，多次频繁的刷新，会导致弹出手机验证码重复登录，由于上次的失败，设置了延时，效果还可以，只是数据总会有延迟模拟点击，无效，仍然会自动登出 采用重复登录的方式，递归实现！ 综上所述，我们可以将交易流程如此划分： WY数据.jpg 成果演示 WY演示.jpg Tips:本文仅做思路分享，切勿用在实际生产环境！","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"网银","slug":"网银","permalink":"https://sbcoder.cn/tags/网银/"},{"name":"selenium","slug":"selenium","permalink":"https://sbcoder.cn/tags/selenium/"}]}]}