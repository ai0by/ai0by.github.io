{"meta":{"title":"风向标 | 分享与创造","subtitle":null,"description":null,"author":"ai0by","url":"https://sbcoder.cn","root":"/"},"pages":[{"title":"api","date":"2019-04-03T02:17:31.000Z","updated":"2019-04-03T03:55:29.301Z","comments":true,"path":"api/index.html","permalink":"https://sbcoder.cn/api/index.html","excerpt":"","text":"微博图床-远程图片上传api介绍我们在使用爬虫相关内容的时候，存放图片时往往会遇到图片尺寸过大，存储不方便等问题，这时候，存放在一个永久存储的云上面就很有必要，微博是一个不限流量，全球CDN的图床~微博也是有缺点的，他并不是一个易于管理的图床，仅限于存放图片但不能管理图片，如果希望使用可以管理的图床，可以参考使用自建图床，参考我的:FuliCOSIMG 参数说明以GET的方式提交到：https://api.0161.org/sinaimg/sinaImg.php 传递参数类型：GET POST 参数 数值类型 示例 是否必传 url String https://sbcoder.cn/img/avatar.jpg 是 返回参数类型 ： JSON 演示示例:1&#123;\"large\":\"http://ww2.sinaimg.cn/bmiddle/0062WdSely1g1p8mlciyrg30390120jl.gif\"&#125; PHP DEMO12345678910111213141516171819$data = array( 'url' =&gt; \"https://sbcoder.cn/img/avatar.jpg\", );echo curlPost(\"https://api.0161.org/sinaimg/sinaImg.php\",$data);function curlPost($url,$res)&#123; $curl = curl_init(); curl_setopt($curl, CURLOPT_URL, $url); curl_setopt($curl, CURLOPT_SSL_VERIFYPEER, 0); curl_setopt($curl, CURLOPT_FOLLOWLOCATION, 1); curl_setopt($curl, CURLOPT_AUTOREFERER, 1); curl_setopt($curl, CURLOPT_POST, 1); curl_setopt($curl, CURLOPT_POSTFIELDS, http_build_query($res)); curl_setopt($curl, CURLOPT_TIMEOUT, 30); curl_setopt($curl, CURLOPT_HEADER, 0); curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1); $result = curl_exec($curl); curl_close($curl); return $result;&#125; 二维码生成介绍使用场景很多，简单来讲，给我地址，我给你图，直接将地址放在img标签内即可~ 参数说明以GET的方式提交到：https://api.0161.org/qrcode/qrcode.php 参数 数值类型 示例 是否必传 url String https://sbcoder.cn 是 err String L (L,M,Q,H四种对应容错级别，不传默认L) 否 size String 7 (可以选择1~9999之间的值，对应不同大小，默认7) 否 logo String https://sbcoder.cn/img/avatar.jpg 否 返回值类型 : 直接返回图片 演示直接访问以下地址 12https://api.0161.org/qrcode/qrcode.php?url=https://sbcoder.cnhttps://api.0161.org/qrcode/qrcode.php?url=https://sbcoder.cn&amp;err=L&amp;size=7&amp;logo=https://sbcoder.cn/img/avatar.jpg 示例图片"},{"title":"tags","date":"2019-03-21T05:14:24.000Z","updated":"2019-03-21T05:14:52.718Z","comments":false,"path":"tags/index.html","permalink":"https://sbcoder.cn/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-03-21T07:11:04.000Z","updated":"2019-03-21T07:11:33.520Z","comments":false,"path":"categories/index.html","permalink":"https://sbcoder.cn/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"192TT(192tb)套图吧整站爬虫","slug":"192TT-192tb-套图吧整站爬虫","date":"2019-03-28T08:15:16.000Z","updated":"2019-03-29T06:33:45.818Z","comments":true,"path":"2019/03/28/192tt_Spider.html","link":"","permalink":"https://sbcoder.cn/2019/03/28/192tt_Spider.html","excerpt":"","text":"观察目录结构目标网站：192tb.com网站结构复杂，但也不是太复杂，总体来说都写在导航栏上面了，本以为是new分类下就是所有的文章了，后来发现不是，需要遍历整个导航分类，由于每个分类都有很庞大的资源，因此我决定写成配置文件的形式，建立config.py 1234567891011121314# -*- coding: utf-8 -*-mt = 'https://www.192tb.com/listinfo-1-1.html' # 美图mt1 = 'https://www.192tb.com/meitu/xingganmeinv/' # 性感美女mt2 = 'https://www.192tb.com/meitu/siwameitui/' # 丝袜美腿mt3 = 'https://www.192tb.com/meitu/weimeixiezhen/' # 唯美写真mt4 = 'https://www.192tb.com/meitu/wangluomeinv/' # 网络美女mt5 = 'https://www.192tb.com/meitu/gaoqingmeinv/' # 高清美女mt6 = 'https://www.192tb.com/meitu/motemeinv/' # 模特美女mt7 = 'https://www.192tb.com/meitu/tiyumeinv/' # 体育美女mt8 = 'https://www.192tb.com/meitu/dongmanmeinv/' # 动漫美女mt9 = 'https://www.192tb.com/new/ugirlapp/' # 爱尤物APP/尤果网gc = 'https://www.192tb.com/gc/' # 国产gc1 = 'https://www.192tb.com/gc/bl/' # beautyleg1 顶级分类和二级分类不便多说，这里只是测试并没有收录所有的分类，有兴趣可以自己添加 进入分类页后既是套图封面，从这里可以爬取套图的链接，分类页的底部也是有下一页的选项，可以根据下一页来获取下一个分类页的链接，以此递归，并获取链接 获取到套图链接后发现每个单页面都是需要点击下一张图片来做的，单页面中的图片，使用BeautifulSoup即可轻松获取，由于不知道一套图里面有多少张，我这边使用递归的方式，走到最后一张，即退出递归。 核心代码获取单个套图并下载1234567891011121314151617def getSingleData(url,singleTitle,i = 1): response = requests.get(url) soup = BeautifulSoup(response.text,\"html.parser\") imgUrl = soup.find(id = 'p').find('center').find('img').get('lazysrc') print imgUrl try: j = i + 1 result = '_%s.html'%i in url if result: nextImg = response.url.replace('_%s.html'%i, '_%s.html'%j) else: nextImg = response.url.replace('.html', '_%s.html'%j) # print nextImg downImg(imgUrl,singleTitle,i) getSingleData(nextImg,j) except Exception,e: return 0 获取下一页页面信息123456789101112131415161718192021222324def getPage(url,new = 1,i = 1): print '开始采集第%s页'%i print url response = requests.get(url) soup = BeautifulSoup(response.text,\"html.parser\") for dataUrl in soup.find('div',&#123;'class':'piclist'&#125;).find('ul').find_all('li'): singleDataUrl = 'https://www.192tb.com/'+dataUrl.find('a').get('href') print singleDataUrl try: singleTitle = dataUrl.find('a').find('img').get('alt') except Exception,e: continue print singleTitle getSingleData(singleDataUrl,singleTitle) result = '_%s.html' % i in url j = i + 1 if new != 1: nextPageUrl = url.replace('listinfo-1-%s.html' % i, 'listinfo-1-%s.html' % j) else: if result: nextPageUrl = url.replace('index_%s.html' % i, 'index_%s.html' % j) else: nextPageUrl = url.replace(url, url+'/index_%s.html' % j) getPage(nextPageUrl,new,j) 演示及下载下载地址：Github 192tt演示图","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"福利","slug":"福利","permalink":"https://sbcoder.cn/tags/福利/"}]},{"title":"正则表达式应用，常用取值表（记录）","slug":"正则表达式应用，常用取值表（记录）","date":"2019-03-26T08:11:06.000Z","updated":"2019-03-27T03:50:12.114Z","comments":true,"path":"2019/03/26/Regex_match_note.html","link":"","permalink":"https://sbcoder.cn/2019/03/26/Regex_match_note.html","excerpt":"","text":"正则表达式 查询表 字符 描述 场景 \\ 转义 转义场景 \\ ^ 匹配输入字符串的开始位置。如果设置了 RegExp 对象的 Multiline 属性，^ 也匹配 ‘\\n’ 或 ‘\\r’ 之后的位置。 取a开头的字符串 ^a.* $ 匹配输入字符串的结束位置。如果设置了RegExp 对象的 Multiline 属性，$ 也匹配 ‘\\n’ 或 ‘\\r’ 之前的位置。 取a开头b结尾 ^a.*b$ * 匹配前面的子表达式零次或多次。 zo 能匹配 “z” 以及 “zoo”。 等价于{0,} + 匹配前面的子表达式一次或多次。 ‘zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,} ? 匹配前面的子表达式零次或一次. “do(es)?” 可以匹配 “do” 或 “does” 中的”do” 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。 ‘o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。 {n,} n 是一个非负整数。至少匹配n 次。 ‘o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。’o{1,}’ 等价于 ‘o+’。’o{0,}’ 则等价于 ‘o*’ {n,m} m 和 n 均为非负整数，其中n &lt;= m。最少匹配 n 次且最多匹配 m 次。 “o{1,3}” 将匹配 “fooooood” 中的前三个 o。’o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。 ? 当 该字符紧跟在任何一个其他限制符 (*, +, ?, {n}, {n,}, {n,m}) 后面时，匹配模式是非贪婪的。 非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。对于字符串 “oooo”，’o+?’ 将匹配单个 “o”，而 ‘o+’ 将匹配所有 ‘o’。 . 匹配除 “\\n” 之外的任何单个字符。 要匹配包括 ‘\\n’ 在内的任何字符，请使用象 ‘[.\\n]’ 的模式。 x&#124;y 匹配 x 或 y。 ‘z&#124;food’ 能匹配 “z” 或 “food”。’(z&#124;f)ood’ 则匹配 “zood” 或 “food”。 [xyz] 字符集合。匹配所包含的任意一个字符。 ‘[abc]’可以匹配 “plain” 中的 ‘a’。 [^xyz] 取反，匹配未包含的任意字符。 ‘?[^abc]’ 可以匹配 “plain” 中的’p’。 [a-z] 字符范围。匹配指定范围内的任意字符。 ‘[a-z]’ 可以匹配 ‘a’ 到 ‘z’ 范围内的任意小写字母字符。 \\b 匹配一个单词边界，也就是指单词和空格间的位置。 ‘er\\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。 \\B 匹配非单词边界 ‘er\\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。 \\cx 匹配由 x 指明的控制字符。 \\cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。 \\d 匹配一个数字字符 等价于 [0-9]。 \\D 匹配一个非数字字符。 等价于 [^0-9]。 \\f 匹配一个换页符。 等价于 \\x0c 和 \\cL。 \\n 匹配一个换行符。 等价于 \\x0a 和 \\cJ。 \\r 匹配一个回车符。 等价于 \\x0d 和 \\cM。 \\s 匹配任何空白字符，包括空格、制表符、换页符等等。 等价于 [ \\f\\n\\r\\t\\v]。 \\S 匹配任何非空白字符。 等价于 [^ \\f\\n\\r\\t\\v]。 \\t 匹配一个制表符。 等价于 \\x09 和 \\cI。 \\v 匹配一个垂直制表符。 等价于 \\x0b 和 \\cK。 \\w 匹配包括下划线的任何单词字符。 等价于’[A-Za-z0-9_]’。 \\W 匹配任何非单词字符。 等价于 ‘[^A-Za-z0-9_]’。 \\xn 匹配十六进制数 ‘\\x41’ 匹配 “A”。’\\x041’ 则等价于 ‘\\x04’ &amp; “1”。正则表达式中可以使用 ASCII 编码。. \\num 匹配 一个正整数。对所获取的匹配的引用。 ‘(.)\\1’ 匹配两个连续的相同字符。 \\n 标识一个八进制转义值或一个向后引用。 如果 \\n 之前至少 n 个获取的子表达式，则 n 为向后引用。否则，如果 n 为八进制数字 (0-7)，则 n 为一个八进制转义值。 \\nm 标 识一个八进制转义值或一个向后引用。 如果 \\nm 之前至少有 nm 个获得子表达式，则 nm 为向后引用。如果 \\nm 之前至少有 n 个获取，则 n 为一个后跟文字 m 的向后引用。如果前面的条件都不满足，若 n 和 m 均为八进制数字 (0-7)，则 \\nm 将匹配八进制转义值 nm。 \\nml 匹配八进制数 如果 n 为八进制数字 (0-3)，且 m 和 l 均为八进制数字 (0-7)，则匹配八进制转义值 nml。 \\un 匹配 n，其中 n 是一个用四个十六进制数字表示的 Unicode 字符。 \\u00A9 匹配版权符号 。 常用案例演示123# -*- coding:utf-8 -*-import restr1 = 'ai0by123' 提取a开头的字符串1regexStr = \"^a.*\" 提取a开头b结尾字符串1regexStr = \"^a.*3$\" 提取最右边符合条件的值,贪婪1regexStr = \".*(a.*b).*\" # 贪婪，取a到b之间，右边开始取，取最右边符合条件的 提取最左边符合条件的值，非贪婪1regexStr = \".*?(a.*?b).*\" # 非贪婪，取a到b之间的值含a和b，从左往右只取一次 提取符合集合内的值，或运算1regexStr = \"((ai00000by|ai0by)123)\" # 或运算，符合其中一种即可 提取出生日期123456str1 = 'XXX 出生于2008年12月6日'str1 = 'XXX 出生于2008/12/6'str1 = 'XXX 出生于2008-12-6'str1 = 'XXX 出生于2008-12-06'str1 = 'XXX 出生于2008-12'regexStr = \".*出生于(\\d&#123;4&#125;[年/-]\\d&#123;1,2&#125;([月/-]\\d&#123;1,2&#125;|[月/-]$|$))\" 提取图片url,其他网站同理1234567str1 = '地址：https://www.ttbcdn.com/d/file/p/2018-02-17/g4edlvxmmyi9627.jpg'# 取整串地址regexStr = \".*https.*jpg$\"# 取XXX.jpg png gif 等regexStr = \".*/(.*.(jpg|gif|png))$\"# 取2018-02-17/g4edlvxmmyi9627.jpg png gif等regexStr = \".*/(\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;/(.*.(jpg|gif|png)))$\" 收尾提取字符串12345reMatch = re.match(regexStr,str1)if reMatch: print (reMatch.group(1))else: print('No')","categories":[{"name":"note","slug":"note","permalink":"https://sbcoder.cn/categories/note/"}],"tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"https://sbcoder.cn/tags/正则表达式/"},{"name":"笔记","slug":"笔记","permalink":"https://sbcoder.cn/tags/笔记/"}]},{"title":"岛国推特妹子图爬虫","slug":"岛国推特妹子图爬虫","date":"2019-03-22T03:54:58.000Z","updated":"2019-03-22T05:11:11.990Z","comments":true,"path":"2019/03/22/Japan_Twitter_Spider.html","link":"","permalink":"https://sbcoder.cn/2019/03/22/Japan_Twitter_Spider.html","excerpt":"","text":"LOC的大佬们分享了一个网站，收集了很多岛国的妹子图和她们的推特地址：岛国妹子推特推特不是很感兴趣，就爬一下图片好了~ 爬虫介绍爬虫环境： Python2.7.9 可更替为3，自行更替 BeautifulSoup4 requests 代码：12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-from bs4 import BeautifulSoupimport requestsimport urllib2import randomdef spy(url): req = urllib2.Request(url) req = urllib2.urlopen(req) page = req.read() soup = BeautifulSoup(page, \"html.parser\") for imgSoup in soup.find_all('div', &#123;\"class\": \"row\"&#125;): for i in imgSoup.find_all('div', &#123;'class': 'photo'&#125;): for j in i.find('div', &#123;'class': 'photo-link-outer'&#125;).find('a').find_all('img'): img = j.get(\"src\") print img str = random.sample('zyxwvutsrqponmlkjihgfedcba', 6) downImg(img, str) nexturl = soup.find('p',&#123;'class':'go-to-next-page'&#125;) nexturl = nexturl.find('a').get('href') pageurl = \"http://jigadori.fkoji.com\"+nexturl spy(pageurl)def downImg(img,m): try: r = requests.get(img) except Exception , e: print \"图片获取失败\" return with open('./img/good%s.jpg' % m, 'wb') as f: f.write(r.content)if __name__ == '__main__': url = \"http://jigadori.fkoji.com\" spy(url) 整体思路看一下，网页构造，发现首页底部有下一页标签，BeautifulSoup取Class取值递归获取下一页地址图片同上整体难度不高，有兴趣的可以拿这个网站练练手~ 演示截图 演示数据1 演示数据2","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"福利","slug":"福利","permalink":"https://sbcoder.cn/tags/福利/"}]},{"title":"Python+selenium针对网银控件过登录取数据","slug":"Python-selenium针对网银控件过登录取数据","date":"2019-03-21T08:53:30.000Z","updated":"2019-03-22T03:10:52.997Z","comments":true,"path":"2019/03/21/Python_WY_Spider.html","link":"","permalink":"https://sbcoder.cn/2019/03/21/Python_WY_Spider.html","excerpt":"","text":"Python获取WY数据的方法总结： 由于WY控件加密方式相比较一般网站比较特殊，需在驱动层进行操作 由于WY控件的特殊性，获取交易数据的时候需要传递KEY (以CCB为例) 由于取数据时的问题，所有操作均应该在驱动层完成 要想实现实时更新数据，需要不断地登录，大部分WY都有强制退出操作 理清思路登录的过程中，由于安全控件的限制，需要绕过登录限制，此处思路借鉴了 爬虫应对银行安全控件，由此可知，需要绕过登陆限制需从驱动层入手 两种方案 Python可以使用 Win32api 模块来模拟键盘指令，类似于按键精灵的概念 Python使用 Photomjs 无界面浏览器配合Selenium Webdirver 尝试使用Win32api时，由于需要配合鼠标操作，需要获取句柄坐标，且开发难度较高，尝试更换另一种方式更换 Photomjs ，模拟登录时发现 Photomjs 并没有附带安全控件，所传输的值不会自动加密，尝试更换为 ChromDriver使用 ChromDriver 尝试模拟登陆 123456username = browser.find_element_by_name('USERID')username.send_keys(username)password = browser.find_element_by_name('LOGPASS')password.send_keys(password)tjButton = browser.find_element_by_id('loginButton')tjButton.click() 登录成功！ 当越过了登录后就需要获取交易信息，交易信息这一块，CCB的查询地址附带了一个SKEY，每次查询信息的时候都需要一个SKEY验证，如果不正确将不会返回正确的结果！如何获取SKEY，涉及到WY的信息，这里不便细说（PS：细心地同学一定可以找到） 取到SKEY后即可构造查询地址，然后使用WebDriver模拟访问需要注意的一点是，WY的大部分数据均是用iframe嵌套的，因此需要多处过iframe 演示代码1234# 定位到iframeiframe = browser.find_element_by_id(\"iframe\")# 切换到iframebrowser.switch_to_frame(iframe) 取数据这一块不多说，每个WY也都不同获取到数据后就是数据处理了，根据系统不同，我这里直接使用Python向固定地址POST传值 取数据后为了获取实时数据，需要定时向固定地址提交数据，大部分的WY都有长时间自动登出的骚操作，对此，思路也很多 大致几个想法 自动刷新，保持登录状态 重复登录，更换SKEY，反复操作 模拟点击，保持登录状态 自动刷新方案在一开始就失败了，多次频繁的刷新，会导致弹出手机验证码重复登录，由于上次的失败，设置了延时，效果还可以，只是数据总会有延迟模拟点击，无效，仍然会自动登出 采用重复登录的方式，递归实现！ 综上所述，我们可以将交易流程如此划分： WY数据.jpg 成果演示 WY演示.jpg Tips:本文仅做思路分享，切勿用在实际生产环境！","categories":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://sbcoder.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://sbcoder.cn/tags/爬虫/"},{"name":"网银","slug":"网银","permalink":"https://sbcoder.cn/tags/网银/"},{"name":"selenium","slug":"selenium","permalink":"https://sbcoder.cn/tags/selenium/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-03-21T03:57:26.875Z","updated":"2019-03-25T05:12:59.453Z","comments":true,"path":"2019/03/21/Hello_world.html","link":"","permalink":"https://sbcoder.cn/2019/03/21/Hello_world.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}