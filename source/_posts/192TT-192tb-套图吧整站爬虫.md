---
title: 192TT(192tb)套图吧整站爬虫
date: 2019-03-28 16:15:16
categories: 
- Python
tags: [Python,爬虫,福利]
urlname: 192tt_Spider
---

## 观察目录结构

目标网站：192tb.com
网站结构复杂，但也不是太复杂，总体来说都写在导航栏上面了，本以为是new分类下就是所有的文章了，后来发现不是，需要遍历整个导航分类，由于每个分类都有很庞大的资源，因此我决定写成配置文件的形式，建立config.py

```python
# -*- coding: utf-8 -*-
mt  = 'https://www.192tb.com/listinfo-1-1.html'                  # 美图
mt1 = 'https://www.192tb.com/meitu/xingganmeinv/'  # 性感美女
mt2 = 'https://www.192tb.com/meitu/siwameitui/'    # 丝袜美腿
mt3 = 'https://www.192tb.com/meitu/weimeixiezhen/' # 唯美写真
mt4 = 'https://www.192tb.com/meitu/wangluomeinv/'  # 网络美女
mt5 = 'https://www.192tb.com/meitu/gaoqingmeinv/'  # 高清美女
mt6 = 'https://www.192tb.com/meitu/motemeinv/'     # 模特美女
mt7 = 'https://www.192tb.com/meitu/tiyumeinv/'     # 体育美女
mt8 = 'https://www.192tb.com/meitu/dongmanmeinv/'  # 动漫美女
mt9 = 'https://www.192tb.com/new/ugirlapp/'        # 爱尤物APP/尤果网

gc  = 'https://www.192tb.com/gc/'                   # 国产
gc1 = 'https://www.192tb.com/gc/bl/' # beautyleg1

```
顶级分类和二级分类不便多说，这里只是测试并没有收录所有的分类，有兴趣可以自己添加

进入分类页后既是套图封面，从这里可以爬取套图的链接，分类页的底部也是有下一页的选项，可以根据下一页来获取下一个分类页的链接，以此递归，并获取链接

获取到套图链接后发现每个单页面都是需要点击下一张图片来做的，单页面中的图片，使用BeautifulSoup即可轻松获取，由于不知道一套图里面有多少张，我这边使用递归的方式，走到最后一张，即退出递归。

## 核心代码

### 获取单个套图并下载
```python
def getSingleData(url,singleTitle,i = 1):
    response = requests.get(url)
    soup = BeautifulSoup(response.text,"html.parser")
    imgUrl = soup.find(id = 'p').find('center').find('img').get('lazysrc')
    print imgUrl
    try:
        j = i + 1
        result = '_%s.html'%i in url
        if result:
            nextImg = response.url.replace('_%s.html'%i, '_%s.html'%j)
        else:
            nextImg = response.url.replace('.html', '_%s.html'%j)
        # print nextImg
        downImg(imgUrl,singleTitle,i)
        getSingleData(nextImg,j)
    except Exception,e:
        return 0
```
### 获取下一页页面信息
```python
def getPage(url,new = 1,i = 1):
    print '开始采集第%s页'%i
    print url
    response = requests.get(url)
    soup = BeautifulSoup(response.text,"html.parser")
    for dataUrl in soup.find('div',{'class':'piclist'}).find('ul').find_all('li'):
        singleDataUrl = 'https://www.192tb.com/'+dataUrl.find('a').get('href')
        print singleDataUrl
        try:
            singleTitle = dataUrl.find('a').find('img').get('alt')
        except Exception,e:
            continue
        print singleTitle
        getSingleData(singleDataUrl,singleTitle)
    result = '_%s.html' % i in url
    j = i + 1
    if new != 1:
        nextPageUrl = url.replace('listinfo-1-%s.html' % i, 'listinfo-1-%s.html' % j)
    else:
        if result:
            nextPageUrl = url.replace('index_%s.html' % i, 'index_%s.html' % j)
        else:
            nextPageUrl = url.replace(url, url+'/index_%s.html' % j)
    getPage(nextPageUrl,new,j)
```

## 演示及下载

下载地址：[Github](https://github.com/ai0by/ai0by-spider/tree/master/192tt)

![192tt演示图](http://fulicos.sbcoder.cn/2019/03/29/5c9dba3205190.png)